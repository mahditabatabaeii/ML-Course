{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbPvq3jVLTds",
        "outputId": "6ff392c7-6b66-44f5-947e-22a4dc79e432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the transformations for the training and test data\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "# Load the CIFAR-10 training and test datasets\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "# Calculate the sizes for training and validation sets\n",
        "train_size = int(0.8 * len(trainset))\n",
        "val_size = len(trainset) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_subset, val_subset = random_split(trainset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders for training, validation, and test sets\n",
        "trainloader = DataLoader(train_subset, batch_size=100, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_subset, batch_size=100, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "JsnYBPRsLj8q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from model import CIFAR10Classifier  # Ensure your model.py has a class named CIFAR10Classifier\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = CIFAR10Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "LSJi9i8BLky0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Train the model\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Print training progress\n",
        "        if i % 200 == 199:  # Print every 100 mini-batches\n",
        "            print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Loss: {running_loss / 100:.4f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(valloader)\n",
        "    val_accuracy = 100 * correct / total\n",
        "    print(f\"[Epoch {epoch + 1}] Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YDLSsNfLms8",
        "outputId": "b2118ca8-38df-47b3-b20a-dc07d751a68d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1374: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1, Batch 200] Loss: 3.7449\n",
            "[Epoch 1, Batch 400] Loss: 3.1886\n",
            "[Epoch 1] Validation Loss: 1.3695, Validation Accuracy: 51.96%\n",
            "[Epoch 2, Batch 200] Loss: 2.9366\n",
            "[Epoch 2, Batch 400] Loss: 2.8768\n",
            "[Epoch 2] Validation Loss: 1.2279, Validation Accuracy: 57.89%\n",
            "[Epoch 3, Batch 200] Loss: 2.7157\n",
            "[Epoch 3, Batch 400] Loss: 2.6778\n",
            "[Epoch 3] Validation Loss: 1.1520, Validation Accuracy: 60.05%\n",
            "[Epoch 4, Batch 200] Loss: 2.5553\n",
            "[Epoch 4, Batch 400] Loss: 2.5356\n",
            "[Epoch 4] Validation Loss: 1.1002, Validation Accuracy: 61.75%\n",
            "[Epoch 5, Batch 200] Loss: 2.4435\n",
            "[Epoch 5, Batch 400] Loss: 2.4566\n",
            "[Epoch 5] Validation Loss: 1.0855, Validation Accuracy: 62.65%\n",
            "[Epoch 6, Batch 200] Loss: 2.3580\n",
            "[Epoch 6, Batch 400] Loss: 2.3660\n",
            "[Epoch 6] Validation Loss: 1.0414, Validation Accuracy: 63.89%\n",
            "[Epoch 7, Batch 200] Loss: 2.2894\n",
            "[Epoch 7, Batch 400] Loss: 2.2959\n",
            "[Epoch 7] Validation Loss: 1.0393, Validation Accuracy: 63.53%\n",
            "[Epoch 8, Batch 200] Loss: 2.2673\n",
            "[Epoch 8, Batch 400] Loss: 2.2165\n",
            "[Epoch 8] Validation Loss: 1.0260, Validation Accuracy: 64.57%\n",
            "[Epoch 9, Batch 200] Loss: 2.1942\n",
            "[Epoch 9, Batch 400] Loss: 2.1946\n",
            "[Epoch 9] Validation Loss: 1.0136, Validation Accuracy: 64.62%\n",
            "[Epoch 10, Batch 200] Loss: 2.1221\n",
            "[Epoch 10, Batch 400] Loss: 2.1391\n",
            "[Epoch 10] Validation Loss: 0.9866, Validation Accuracy: 65.36%\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "xlETAzZGLrXq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOXz47phOIXV",
        "outputId": "1e823379-832f-4d84-a223-46295c71977c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 64.41%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question **5**"
      ],
      "metadata": {
        "id": "VkZx4B38OlzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTzY-p9vOjhX",
        "outputId": "3608dbab-3091-4301-f8ca-30c391385e75"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def fgsm_attack(model, loss, images, labels, epsilon):\n",
        "    images = images.clone().detach().requires_grad_(True)\n",
        "    outputs = model(images)\n",
        "    model.zero_grad()\n",
        "    cost = loss(outputs, labels).to(images.device)\n",
        "    cost.backward()\n",
        "    attack_images = images + epsilon * images.grad.sign()\n",
        "    attack_images = torch.clamp(attack_images, 0, 1)\n",
        "    return attack_images"
      ],
      "metadata": {
        "id": "4K1Q6J_uO3eP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from model import CIFAR10Classifier  # Ensure your model.py has a class named CIFAR10Classifier\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = CIFAR10Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Adversarial training parameters\n",
        "epsilon = 0.1  # perturbation amount\n",
        "\n",
        "# Train the model with adversarial examples\n",
        "for epoch in range(10):  # Number of epochs\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Generate adversarial examples\n",
        "        adv_inputs = fgsm_attack(model, criterion, inputs, labels, epsilon)\n",
        "\n",
        "        # Forward + backward + optimize (on both original and adversarial examples)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        adv_outputs = model(adv_inputs)\n",
        "        adv_loss = criterion(adv_outputs, labels)\n",
        "        total_loss = (loss + adv_loss) / 2\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += total_loss.item()\n",
        "\n",
        "        # Print training progress\n",
        "        if i % 200 == 199:  # Print every 100 mini-batches\n",
        "            print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Loss: {running_loss / 100:.4f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(valloader)\n",
        "    val_accuracy = 100 * correct / total\n",
        "    print(f\"[Epoch {epoch + 1}] Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "print('Finished Training with Adversarial Privacy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMcOrj5mO7-X",
        "outputId": "2b000a18-865e-4197-e0f3-3ec81c6f2984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1, Batch 200] Loss: 4.0356\n",
            "[Epoch 1, Batch 400] Loss: 3.6730\n",
            "[Epoch 1] Validation Loss: 1.4330, Validation Accuracy: 49.18%\n",
            "[Epoch 2, Batch 200] Loss: 3.5251\n",
            "[Epoch 2, Batch 400] Loss: 3.4333\n",
            "[Epoch 2] Validation Loss: 1.2781, Validation Accuracy: 55.71%\n",
            "[Epoch 3, Batch 200] Loss: 3.3382\n",
            "[Epoch 3, Batch 400] Loss: 3.2985\n",
            "[Epoch 3] Validation Loss: 1.1776, Validation Accuracy: 58.40%\n",
            "[Epoch 4, Batch 200] Loss: 3.2165\n",
            "[Epoch 4, Batch 400] Loss: 3.2258\n",
            "[Epoch 4] Validation Loss: 1.1632, Validation Accuracy: 59.87%\n",
            "[Epoch 5, Batch 200] Loss: 3.1439\n",
            "[Epoch 5, Batch 400] Loss: 3.1795\n",
            "[Epoch 5] Validation Loss: 1.1174, Validation Accuracy: 61.21%\n",
            "[Epoch 6, Batch 200] Loss: 3.0947\n",
            "[Epoch 6, Batch 400] Loss: 3.1060\n",
            "[Epoch 6] Validation Loss: 1.0866, Validation Accuracy: 61.87%\n",
            "[Epoch 7, Batch 200] Loss: 3.0383\n",
            "[Epoch 7, Batch 400] Loss: 3.0477\n",
            "[Epoch 7] Validation Loss: 1.0852, Validation Accuracy: 62.24%\n",
            "[Epoch 8, Batch 200] Loss: 2.9901\n",
            "[Epoch 8, Batch 400] Loss: 2.9954\n",
            "[Epoch 8] Validation Loss: 1.0569, Validation Accuracy: 63.71%\n",
            "[Epoch 9, Batch 200] Loss: 2.9402\n",
            "[Epoch 9, Batch 400] Loss: 2.9590\n",
            "[Epoch 9] Validation Loss: 1.0375, Validation Accuracy: 64.14%\n",
            "[Epoch 10, Batch 200] Loss: 2.8949\n",
            "[Epoch 10, Batch 400] Loss: 2.9328\n",
            "[Epoch 10] Validation Loss: 1.0337, Validation Accuracy: 64.16%\n",
            "Finished Training with Adversarial Privacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "PATH = './cifar_net_adversarial.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "aITnztr7PB8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LW__23xoPDgo",
        "outputId": "2ba98fb5-5381-49fd-908a-caf2c0f53eb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 64.55%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Differential"
      ],
      "metadata": {
        "id": "7i8BAydGSPiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opacus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtnir45uTKM-",
        "outputId": "bda2b264-5186-447a-c068-36020e53e583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opacus\n",
            "  Downloading opacus-1.4.1-py3-none-any.whl (226 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/226.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from opacus) (1.25.2)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from opacus) (2.3.0+cu121)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.10/dist-packages (from opacus) (1.11.4)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from opacus) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->opacus) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->opacus) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0->opacus) (1.3.0)\n",
            "Installing collected packages: opacus\n",
            "Successfully installed opacus-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from opacus import PrivacyEngine"
      ],
      "metadata": {
        "id": "RTCSt3WES9lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Add L2 regularization\n",
        "\n",
        "# Initialize Privacy Engine\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "# Attach the privacy engine to the optimizer\n",
        "model, optimizer, trainloader = privacy_engine.make_private(\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=trainloader,\n",
        "    noise_multiplier=1.1,\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "# Training the model with Differential Privacy\n",
        "for epoch in range(10):  # number of epochs\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Add noise to inputs for privacy\n",
        "        inputs = inputs + torch.randn_like(inputs) * 0.1\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Apply temperature scaling to the outputs\n",
        "        temperature = 2.0\n",
        "        outputs = outputs / temperature\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Print training progress\n",
        "        if i % 200 == 199:  # Print every 100 mini-batches\n",
        "            print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Loss: {running_loss / 100:.4f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Apply temperature scaling to the outputs\n",
        "            outputs = outputs / temperature\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(valloader)\n",
        "    val_accuracy = 100 * correct / total\n",
        "    print(f\"[Epoch {epoch + 1}] Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27Eohh1-SPKu",
        "outputId": "9e1309b2-c130-4719-cbeb-11682a88727b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1374: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1, Batch 200] Loss: 4.5444\n",
            "[Epoch 1, Batch 400] Loss: 4.3894\n",
            "[Epoch 1] Validation Loss: 2.0943, Validation Accuracy: 26.24%\n",
            "[Epoch 2, Batch 200] Loss: 4.3135\n",
            "[Epoch 2, Batch 400] Loss: 4.2300\n",
            "[Epoch 2] Validation Loss: 1.9811, Validation Accuracy: 29.84%\n",
            "[Epoch 3, Batch 200] Loss: 4.1867\n",
            "[Epoch 3, Batch 400] Loss: 4.1414\n",
            "[Epoch 3] Validation Loss: 1.9138, Validation Accuracy: 32.48%\n",
            "[Epoch 4, Batch 200] Loss: 4.1198\n",
            "[Epoch 4, Batch 400] Loss: 4.0723\n",
            "[Epoch 4] Validation Loss: 1.8793, Validation Accuracy: 33.66%\n",
            "[Epoch 5, Batch 200] Loss: 4.0642\n",
            "[Epoch 5, Batch 400] Loss: 4.0534\n",
            "[Epoch 5] Validation Loss: 1.8392, Validation Accuracy: 34.86%\n",
            "[Epoch 6, Batch 200] Loss: 4.0377\n",
            "[Epoch 6, Batch 400] Loss: 4.0138\n",
            "[Epoch 6] Validation Loss: 1.8117, Validation Accuracy: 35.63%\n",
            "[Epoch 7, Batch 200] Loss: 4.0145\n",
            "[Epoch 7, Batch 400] Loss: 4.0077\n",
            "[Epoch 7] Validation Loss: 1.8010, Validation Accuracy: 36.00%\n",
            "[Epoch 8, Batch 200] Loss: 3.9811\n",
            "[Epoch 8, Batch 400] Loss: 3.9993\n",
            "[Epoch 8] Validation Loss: 1.7758, Validation Accuracy: 37.00%\n",
            "[Epoch 9, Batch 200] Loss: 3.9916\n",
            "[Epoch 9, Batch 400] Loss: 3.9993\n",
            "[Epoch 9] Validation Loss: 1.7524, Validation Accuracy: 37.38%\n",
            "[Epoch 10, Batch 200] Loss: 3.9535\n",
            "[Epoch 10, Batch 400] Loss: 3.9710\n",
            "[Epoch 10] Validation Loss: 1.7539, Validation Accuracy: 37.76%\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "PATH = './cifar_net_differential.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "822CWduQSZnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZQHhKKXSc84",
        "outputId": "9be333e9-0106-48fb-eafe-8c9120eb80b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 38.41%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PATE"
      ],
      "metadata": {
        "id": "DfbAYDKaUQHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import random_split, DataLoader, Subset\n",
        "from model import CIFAR10Classifier\n",
        "import numpy as np\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "# Split the dataset into disjoint subsets for teacher models\n",
        "num_teachers = 10\n",
        "teacher_subsets = random_split(trainset, [len(trainset) // num_teachers] * num_teachers)\n",
        "\n",
        "# Train each teacher model\n",
        "teacher_models = []\n",
        "for i, subset in enumerate(teacher_subsets):\n",
        "    trainloader = DataLoader(subset, batch_size=100, shuffle=True, num_workers=2)\n",
        "\n",
        "    model = CIFAR10Classifier().to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(5):  # Number of epochs for teacher models\n",
        "        model.train()\n",
        "        for data in trainloader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    teacher_models.append(model)\n",
        "    print(f\"Teacher {i + 1} trained\")\n",
        "\n",
        "print(\"All teacher models trained\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk3NnShMUP58",
        "outputId": "f688e61b-5b39-40b0-c1c1-5dafe1d7afb1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Teacher 1 trained\n",
            "Teacher 2 trained\n",
            "Teacher 3 trained\n",
            "Teacher 4 trained\n",
            "Teacher 5 trained\n",
            "Teacher 6 trained\n",
            "Teacher 7 trained\n",
            "Teacher 8 trained\n",
            "Teacher 9 trained\n",
            "Teacher 10 trained\n",
            "All teacher models trained\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student_labels = []\n",
        "student_data = []\n",
        "\n",
        "# Aggregating predictions\n",
        "for data in DataLoader(trainset, batch_size=100, shuffle=False):\n",
        "    inputs, labels = data\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    # Collect predictions from all teacher models\n",
        "    teacher_preds = torch.zeros((num_teachers, inputs.size(0), 10)).to(device)\n",
        "    for i, model in enumerate(teacher_models):\n",
        "        outputs = model(inputs)\n",
        "        teacher_preds[i] = outputs\n",
        "\n",
        "    # Aggregate predictions and add noise\n",
        "    aggregated_preds = torch.sum(teacher_preds, dim=0)\n",
        "    noisy_preds = aggregated_preds + torch.randn_like(aggregated_preds) * 0.1  # Adding noise\n",
        "    student_labels.append(torch.argmax(noisy_preds, dim=1))\n",
        "    student_data.append(inputs)\n",
        "\n",
        "student_labels = torch.cat(student_labels).to(device)\n",
        "student_data = torch.cat(student_data).to(device)\n",
        "\n",
        "student_model = CIFAR10Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(student_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "student_dataset = torch.utils.data.TensorDataset(student_data, student_labels)\n",
        "student_loader = DataLoader(student_dataset, batch_size=100, shuffle=True)\n",
        "\n",
        "for epoch in range(10):\n",
        "    student_model.train()\n",
        "    running_loss = 0.0\n",
        "    for data in student_loader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = student_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Print training progress\n",
        "        if i % 100 == 99:\n",
        "            print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Loss: {running_loss / 100:.4f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training Student Model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meYz1DOhV509",
        "outputId": "dafeea72-3b1b-4a04-dcde-1436a9b6fdcb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training Student Model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained student model\n",
        "PATH = './cifar_student_net.pth'\n",
        "torch.save(student_model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "G1RHL0C6U63q"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the student model\n",
        "student_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = student_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy of the student model on the 10000 test images: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlLb2D0CU-0y",
        "outputId": "e85a124a-85a8-447c-cf8c-bcda85f705b8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the student model on the 10000 test images: 49.32%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question **6**"
      ],
      "metadata": {
        "id": "ZyFk38sCQJF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1\n",
        "# Load the baseline model\n",
        "baseline_model = CIFAR10Classifier().to(device)\n",
        "baseline_model.load_state_dict(torch.load('./cifar_net.pth'))\n",
        "baseline_model.eval()\n",
        "\n",
        "# Load the adversarially trained model\n",
        "modified_model = CIFAR10Classifier().to(device)\n",
        "modified_model.load_state_dict(torch.load('./cifar_student_net.pth'))\n",
        "modified_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMkWC1FZQI0C",
        "outputId": "50beeb02-b3a5-4ec7-9ed7-84d0324f476d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CIFAR10Classifier(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
              "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
              "  (fc1): Linear(in_features=6272, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2\n",
        "# Split the training data into seen and unseen datasets\n",
        "train_size = int(0.8 * len(trainset))\n",
        "val_size = len(trainset) - train_size\n",
        "seen_dataset, unseen_dataset = random_split(trainset, [train_size, val_size])\n",
        "\n",
        "# Combine the unseen dataset with the test data to form the final unseen dataset\n",
        "final_unseen_dataset = torch.utils.data.ConcatDataset([unseen_dataset, testset])\n",
        "\n",
        "# Create DataLoaders for the seen and unseen datasets\n",
        "seen_loader = DataLoader(seen_dataset, batch_size=100, shuffle=True, num_workers=2)\n",
        "unseen_loader = DataLoader(final_unseen_dataset, batch_size=100, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "WEbtCu-cQPsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3\n",
        "num_shadow_models = 5\n",
        "shadow_models = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    model = CIFAR10Classifier().to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "    shadow_trainset, shadow_valset = random_split(trainset, [train_size, val_size])\n",
        "    shadow_loader = DataLoader(shadow_trainset, batch_size=100, shuffle=True, num_workers=2)\n",
        "\n",
        "    # Training the shadow model\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        for data in shadow_loader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    shadow_models.append(model)\n",
        "    print(f\"Shadow model {i+1} trained.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fjSmfn-QVL9",
        "outputId": "03b7068f-46ac-405e-f7b0-1da66ac2ef3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shadow model 1 trained.\n",
            "Shadow model 2 trained.\n",
            "Shadow model 3 trained.\n",
            "Shadow model 4 trained.\n",
            "Shadow model 5 trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4\n",
        "# Function to get model predictions\n",
        "def get_model_predictions(model, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            inputs, _ = data\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            predictions.append(outputs)\n",
        "    return torch.cat(predictions)\n",
        "\n",
        "# Generate attacker datasets\n",
        "def generate_attacker_dataset(shadow_models, seen_loader, unseen_loader):\n",
        "    shadow_preds_seen = [get_model_predictions(model, seen_loader) for model in shadow_models]\n",
        "    shadow_preds_unseen = [get_model_predictions(model, unseen_loader) for model in shadow_models]\n",
        "\n",
        "    X = torch.cat(shadow_preds_seen + shadow_preds_unseen)\n",
        "    y = torch.cat([torch.ones(len(pred)) for pred in shadow_preds_seen] + [torch.zeros(len(pred)) for pred in shadow_preds_unseen])\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Get attacker datasets for baseline and modified models\n",
        "X_baseline, y_baseline = generate_attacker_dataset(shadow_models, seen_loader, unseen_loader)\n",
        "X_modified, y_modified = generate_attacker_dataset(shadow_models, seen_loader, unseen_loader)\n"
      ],
      "metadata": {
        "id": "0Oc0V2oXRc4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5\n",
        "# Define the attacker model\n",
        "class AttackerModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AttackerModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# Train the attacker model\n",
        "def train_attacker_model(X, y):\n",
        "    attacker_model = AttackerModel().to(device)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(attacker_model.parameters(), lr=0.001)\n",
        "\n",
        "    dataset = torch.utils.data.TensorDataset(X, y)\n",
        "    loader = DataLoader(dataset, batch_size=100, shuffle=True)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        attacker_model.train()\n",
        "        running_loss = 0.0\n",
        "        for data in loader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = attacker_model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Attacker model training epoch {epoch+1} loss: {running_loss/len(loader):.4f}\")\n",
        "\n",
        "    return attacker_model\n",
        "\n",
        "# Train attacker models for baseline and modified models\n",
        "attacker_model_baseline = train_attacker_model(X_baseline, y_baseline)\n",
        "attacker_model_modified = train_attacker_model(X_modified, y_modified)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89QxCeNnaZga",
        "outputId": "72df0f12-7af9-41e5-bc3b-60504ebd79ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attacker model training epoch 1 loss: 0.6394\n",
            "Attacker model training epoch 2 loss: 0.6375\n",
            "Attacker model training epoch 3 loss: 0.6368\n",
            "Attacker model training epoch 4 loss: 0.6365\n",
            "Attacker model training epoch 5 loss: 0.6363\n",
            "Attacker model training epoch 6 loss: 0.6363\n",
            "Attacker model training epoch 7 loss: 0.6362\n",
            "Attacker model training epoch 8 loss: 0.6361\n",
            "Attacker model training epoch 9 loss: 0.6361\n",
            "Attacker model training epoch 10 loss: 0.6360\n",
            "Attacker model training epoch 1 loss: 0.6389\n",
            "Attacker model training epoch 2 loss: 0.6375\n",
            "Attacker model training epoch 3 loss: 0.6370\n",
            "Attacker model training epoch 4 loss: 0.6365\n",
            "Attacker model training epoch 5 loss: 0.6364\n",
            "Attacker model training epoch 6 loss: 0.6363\n",
            "Attacker model training epoch 7 loss: 0.6362\n",
            "Attacker model training epoch 8 loss: 0.6360\n",
            "Attacker model training epoch 9 loss: 0.6360\n",
            "Attacker model training epoch 10 loss: 0.6360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6\n",
        "# Function to evaluate the attacker model\n",
        "def evaluate_attacker_model(attacker_model, X, y):\n",
        "    attacker_model.eval()\n",
        "    dataset = torch.utils.data.TensorDataset(X, y)\n",
        "    loader = DataLoader(dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
        "            outputs = attacker_model(inputs)\n",
        "            predicted = (outputs.squeeze() > 0.5).float()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Attacker Model Accuracy: {accuracy:.2f}%')\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate the attacker models\n",
        "print(\"Evaluating attacker model for baseline model:\")\n",
        "baseline_attacker_accuracy = evaluate_attacker_model(attacker_model_baseline, X_baseline, y_baseline)\n",
        "\n",
        "print(\"Evaluating attacker model for modified model:\")\n",
        "modified_attacker_accuracy = evaluate_attacker_model(attacker_model_modified, X_modified, y_modified)\n",
        "\n",
        "# Compare the MIA accuracy\n",
        "print(f\"Baseline Model MIA Accuracy: {baseline_attacker_accuracy:.2f}%\")\n",
        "print(f\"Modified Model MIA Accuracy: {modified_attacker_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFnv-OFQdMJg",
        "outputId": "6d384abb-7e1b-40bf-d143-3593c1d7fe21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating attacker model for baseline model:\n",
            "Attacker Model Accuracy: 66.67%\n",
            "Evaluating attacker model for modified model:\n",
            "Attacker Model Accuracy: 66.67%\n",
            "Baseline Model MIA Accuracy: 66.67%\n",
            "Modified Model MIA Accuracy: 66.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Newwww"
      ],
      "metadata": {
        "id": "t2RGodRDk2LN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Assuming model.py contains the CIFAR10Classifier definition and necessary imports\n",
        "from model import CIFAR10Classifier\n",
        "\n",
        "# Load the models\n",
        "baseline_model = CIFAR10Classifier()\n",
        "modified_model = CIFAR10Classifier()\n",
        "\n",
        "baseline_model.load_state_dict(torch.load('./cifar_net.pth'))\n",
        "modified_model.load_state_dict(torch.load('./cifar_student_net.pth'))\n",
        "\n",
        "baseline_model.eval()\n",
        "modified_model.eval()\n",
        "\n",
        "# Prepare dataset (assuming CIFAR-10 dataset for illustration)\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Splitting training data into seen (80%) and unseen (20%) data\n",
        "train_indices = list(range(len(train_data)))\n",
        "train_seen_indices, train_unseen_indices = train_test_split(train_indices, test_size=0.2, random_state=42)\n",
        "\n",
        "seen_data = Subset(train_data, train_seen_indices)\n",
        "unseen_data = Subset(train_data, train_unseen_indices)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "\n",
        "# DataLoader for seen and unseen data\n",
        "seen_loader = DataLoader(seen_data, batch_size=64, shuffle=True)\n",
        "unseen_loader = DataLoader(unseen_data, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define a simple attacker model\n",
        "class AttackerModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(AttackerModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# Function to extract features\n",
        "def extract_features(model, loader):\n",
        "    features = []\n",
        "    labels = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            outputs = model(inputs)\n",
        "            features.append(outputs.view(outputs.size(0), -1).cpu().numpy())\n",
        "            labels.append(targets.cpu().numpy())\n",
        "    features = np.concatenate(features, axis=0)\n",
        "    labels = np.concatenate(labels, axis=0)\n",
        "    return features, labels\n",
        "\n",
        "# Extract features from both models\n",
        "baseline_seen_features, _ = extract_features(baseline_model, seen_loader)\n",
        "baseline_unseen_features, _ = extract_features(baseline_model, unseen_loader)\n",
        "modified_seen_features, _ = extract_features(modified_model, seen_loader)\n",
        "modified_unseen_features, _ = extract_features(modified_model, unseen_loader)\n",
        "\n",
        "# Prepare attacker dataset\n",
        "def prepare_attacker_data(seen_features, unseen_features):\n",
        "    seen_labels = np.ones(len(seen_features))\n",
        "    unseen_labels = np.zeros(len(unseen_features))\n",
        "    features = np.concatenate((seen_features, unseen_features), axis=0)\n",
        "    labels = np.concatenate((seen_labels, unseen_labels), axis=0)\n",
        "    return features, labels\n",
        "\n",
        "baseline_features, baseline_labels = prepare_attacker_data(baseline_seen_features, baseline_unseen_features)\n",
        "modified_features, modified_labels = prepare_attacker_data(modified_seen_features, modified_unseen_features)\n",
        "\n",
        "# Train attacker model\n",
        "def train_attacker_model(features, labels):\n",
        "    input_dim = features.shape[1]\n",
        "    attacker_model = AttackerModel(input_dim)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(attacker_model.parameters(), lr=0.001)\n",
        "\n",
        "    dataset = torch.utils.data.TensorDataset(torch.tensor(features, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32).unsqueeze(1))\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        for data, target in loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = attacker_model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return attacker_model\n",
        "\n",
        "baseline_attacker = train_attacker_model(baseline_features, baseline_labels)\n",
        "modified_attacker = train_attacker_model(modified_features, modified_labels)\n",
        "\n",
        "# Evaluate attacker models\n",
        "def evaluate_attacker_model(attacker_model, features, labels):\n",
        "    attacker_model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = attacker_model(torch.tensor(features, dtype=torch.float32))\n",
        "        predictions = (outputs.cpu().numpy() > 0.5).astype(int)\n",
        "    return accuracy_score(labels, predictions)\n",
        "\n",
        "# Extract test features for evaluation\n",
        "baseline_test_features, _ = extract_features(baseline_model, test_loader)\n",
        "modified_test_features, _ = extract_features(modified_model, test_loader)\n",
        "\n",
        "baseline_test_labels = np.zeros(len(baseline_test_features))\n",
        "modified_test_labels = np.zeros(len(modified_test_features))\n",
        "\n",
        "# Evaluate the attacker models on test data\n",
        "baseline_mia_accuracy = evaluate_attacker_model(baseline_attacker, baseline_test_features, baseline_test_labels)\n",
        "modified_mia_accuracy = evaluate_attacker_model(modified_attacker, modified_test_features, modified_test_labels)\n",
        "\n",
        "print(f\"Baseline Model MIA Accuracy: {baseline_mia_accuracy}\")\n",
        "print(f\"Modified Model MIA Accuracy: {modified_mia_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2ynj7ffk38f",
        "outputId": "f030dd83-f5a2-4237-d883-4679c63db667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Baseline Model MIA Accuracy: 0.0\n",
            "Modified Model MIA Accuracy: 0.0005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gonna Die Soon\n",
        "## Everything again"
      ],
      "metadata": {
        "id": "SkQ9eTRbxZ0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset, random_split\n",
        "import torch.nn.functional as F\n",
        "from model import CIFAR10Classifier\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "L9RUyep5xvWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split training data into 80% seen data and 20% unseen data\n",
        "train_size = int(0.8 * len(train_set))\n",
        "remaining_size = len(train_set) - train_size\n",
        "train_subset, remaining_subset = random_split(train_set, [train_size, remaining_size])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQepITLqx168",
        "outputId": "24b19a9c-b172-40f7-b8f0-5c5b5dbfe731"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create loaders for seen and unseen data\n",
        "num_shadow_models = 5\n",
        "seen_size_per_model = train_size // num_shadow_models\n",
        "seen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * seen_size_per_model\n",
        "    end_idx = (i + 1) * seen_size_per_model\n",
        "    seen_indices = torch.arange(start_idx, end_idx)\n",
        "    seen_train_set = Subset(train_subset, seen_indices)\n",
        "    seen_loader = DataLoader(seen_train_set, batch_size=100, shuffle=True)\n",
        "    seen_loaders.append(seen_loader)\n",
        "\n",
        "# Create concatenated unseen data from the remaining 20% of training data and the entire test set\n",
        "unseen_dataset = ConcatDataset([remaining_subset, test_set])\n",
        "unseen_size_per_model = len(unseen_dataset) // num_shadow_models\n",
        "unseen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * unseen_size_per_model\n",
        "    end_idx = (i + 1) * unseen_size_per_model\n",
        "    unseen_indices = torch.arange(start_idx, end_idx)\n",
        "    unseen_subset = Subset(unseen_dataset, unseen_indices)\n",
        "    unseen_loader = DataLoader(unseen_subset, batch_size=100, shuffle=False)\n",
        "    unseen_loaders.append(unseen_loader)\n",
        "\n",
        "test_loader = DataLoader(test_set, batch_size=100, shuffle=False)"
      ],
      "metadata": {
        "id": "xhgfgaNfx6JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the attack model\n",
        "class AttackModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AttackModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.fc4 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = torch.sigmoid(self.fc4(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "u0fMlTSpx9y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MembershipInferenceAttack:\n",
        "    def __init__(self, shadow_model_class, attack_model_class, device='cpu'):\n",
        "        self.shadow_model_class = shadow_model_class\n",
        "        self.attack_model_class = attack_model_class\n",
        "        self.device = device\n",
        "        self.attack_models = {}\n",
        "\n",
        "    def train_shadow_models(self, seen_loaders, num_epochs=10, lr=1e-3):\n",
        "        self.shadow_models = [self.shadow_model_class().to(self.device) for _ in range(len(seen_loaders))]\n",
        "\n",
        "        for i, (shadow_model, seen_loader) in enumerate(zip(self.shadow_models, seen_loaders)):\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.Adam(shadow_model.parameters(), lr=lr)\n",
        "            self._train_model(shadow_model, seen_loader, criterion, optimizer, num_epochs)\n",
        "            print(f'Shadow model {i+1} trained.')\n",
        "\n",
        "    def _train_model(self, model, dataloader, criterion, optimizer, num_epochs):\n",
        "        model.train()\n",
        "        for epoch in range(num_epochs):\n",
        "            running_loss = 0.0\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "            print(f'Epoch {epoch+1}, Loss: {running_loss/len(dataloader):.6f}')\n",
        "\n",
        "    def collect_outputs(self, seen_loaders, unseen_loaders):\n",
        "        self.attack_data = []\n",
        "        self.attack_labels = []\n",
        "\n",
        "        for shadow_model, seen_loader, unseen_loader in zip(self.shadow_models, seen_loaders, unseen_loaders):\n",
        "            self._collect_shadow_model_outputs(shadow_model, seen_loader, label=1)  # in\n",
        "            self._collect_shadow_model_outputs(shadow_model, unseen_loader, label=0)  # out\n",
        "\n",
        "        self.attack_data = torch.cat(self.attack_data).to(self.device)\n",
        "        self.attack_labels = torch.cat(self.attack_labels).to(self.device)\n",
        "\n",
        "    def _collect_shadow_model_outputs(self, model, dataloader, label):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                probabilities = F.softmax(outputs, dim=1)\n",
        "                self.attack_data.append(torch.cat([probabilities, labels.unsqueeze(1).float()], dim=1))\n",
        "                self.attack_labels.append(torch.full((outputs.size(0),), label, dtype=torch.float).to(self.device))\n",
        "\n",
        "    def train_attack_models(self, num_epochs=10, lr=0.001):\n",
        "        for class_label in range(10):  # Assuming 10 classes\n",
        "            class_indices = (self.attack_data[:, -1] == class_label).nonzero().squeeze()\n",
        "            class_data = self.attack_data[class_indices][:, :-1]  # Exclude the last column (class label)\n",
        "            class_labels = self.attack_labels[class_indices].view(-1, 1)  # Ensure labels are the same shape as outputs\n",
        "\n",
        "            attack_dataset = torch.utils.data.TensorDataset(class_data, class_labels)\n",
        "            attack_loader = DataLoader(attack_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "            attack_model = self.attack_model_class().to(self.device)\n",
        "            criterion = nn.BCELoss()\n",
        "            optimizer = optim.Adam(attack_model.parameters(), lr=lr)\n",
        "\n",
        "            self._train_model(attack_model, attack_loader, criterion, optimizer, num_epochs)\n",
        "            self.attack_models[class_label] = attack_model\n",
        "\n",
        "            print(f'Attack model for class {class_label} trained.')\n",
        "\n",
        "    def save_attack_models(self, path):\n",
        "        for class_label, model in self.attack_models.items():\n",
        "            torch.save(model.state_dict(), f'{path}_class_{class_label}.pth')\n",
        "            print(f'Attack model for class {class_label} saved to {path}_class_{class_label}.pth')\n",
        "\n",
        "    def load_attack_models(self, path):\n",
        "        for class_label in range(10):  # Assuming 10 classes\n",
        "            model = self.attack_model_class().to(self.device)\n",
        "            model.load_state_dict(torch.load(f'{path}_class_{class_label}.pth', map_location=self.device))\n",
        "            self.attack_models[class_label] = model\n",
        "            print(f'Attack model for class {class_label} loaded from {path}_class_{class_label}.pth')\n",
        "\n",
        "    def infer_membership(self, seen_loader, unseen_loader, seen_outputs, unseen_outputs, labels):\n",
        "        model_outputs = torch.cat([seen_outputs, unseen_outputs]).to(self.device)\n",
        "\n",
        "        print(\"In Infer : \")\n",
        "        print(f\"model_outputs size: {model_outputs.size()}\")\n",
        "\n",
        "        if isinstance(labels, list):\n",
        "            labels = torch.cat(labels).to(self.device)\n",
        "        else:\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "        print(f\"labels size: {len(labels)}\")\n",
        "\n",
        "        memberships = []\n",
        "        for output, label in zip(model_outputs, labels):\n",
        "            class_label = label.item()\n",
        "            attack_model = self.attack_models[class_label]\n",
        "            membership_pred = attack_model(output.unsqueeze(0)).item()\n",
        "            memberships.append(membership_pred)\n",
        "        print(f\"memberships size: {len(memberships)}\")\n",
        "        return torch.tensor(memberships, device=self.device)\n",
        "\n",
        "    def evaluate_attack_model(self, seen_loader, unseen_loader, target_model):\n",
        "        seen_outputs, labels_seen = self._get_model_outputs(target_model, seen_loader)\n",
        "        unseen_outputs, labels_unseen = self._get_model_outputs(target_model, unseen_loader)\n",
        "\n",
        "        print(f\"Seen outputs size: {seen_outputs.size()}\")\n",
        "        print(f\"Unseen outputs size: {unseen_outputs.size()}\")\n",
        "\n",
        "        attack_data = torch.cat([seen_outputs, unseen_outputs]).to(self.device)\n",
        "        attack_labels = torch.cat([torch.ones(len(seen_outputs)), torch.zeros(len(unseen_outputs))]).to(self.device)\n",
        "\n",
        "        # Ensure labels_seen and labels_unseen are tensors\n",
        "        if isinstance(labels_seen, list):\n",
        "            labels_seen = torch.cat(labels_seen)\n",
        "        if isinstance(labels_unseen, list):\n",
        "            labels_unseen = torch.cat(labels_unseen)\n",
        "\n",
        "        labels = torch.cat([labels_seen, labels_unseen]).to(self.device)\n",
        "\n",
        "        memberships = self.infer_membership(seen_loader, unseen_loader, seen_outputs, unseen_outputs, labels)\n",
        "        membership_preds = (memberships > 0.5).float()\n",
        "        accuracy = (membership_preds == attack_labels).float().mean().item()\n",
        "        return accuracy\n",
        "\n",
        "    def _get_model_outputs(self, model, dataloader):\n",
        "        model.eval()\n",
        "        outputs_list = []\n",
        "        labels_list = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                probabilities = F.softmax(outputs, dim=1)\n",
        "                outputs_list.append(probabilities)\n",
        "                labels_list.append(labels.cpu())  # Ensure labels are moved to CPU for concatenation\n",
        "        return torch.cat(outputs_list), labels_list  # Return labels_list as a list of tensors"
      ],
      "metadata": {
        "id": "cm5Ab-afxXkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the models\n",
        "baseline_model = CIFAR10Classifier().to(device)\n",
        "baseline_model.load_state_dict(torch.load('./cifar_net.pth'))\n",
        "baseline_model.eval()\n",
        "\n",
        "private_model = CIFAR10Classifier().to(device)\n",
        "private_model.load_state_dict(torch.load('./cifar_student_net.pth'))\n",
        "private_model.eval()\n",
        "\n",
        "# Initialize MembershipInferenceAttack\n",
        "mia = MembershipInferenceAttack(CIFAR10Classifier, AttackModel, device)\n",
        "\n",
        "# Train shadow models\n",
        "mia.train_shadow_models(seen_loaders, num_epochs=10)\n",
        "\n",
        "# Collect outputs for attack model\n",
        "mia.collect_outputs(seen_loaders, unseen_loaders)\n",
        "\n",
        "# Train attack models\n",
        "mia.train_attack_models(num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9kZy5StyDCl",
        "outputId": "a41ab61b-1c08-47b9-bfe8-9eb44170be84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.005599\n",
            "Epoch 2, Loss: 1.709452\n",
            "Epoch 3, Loss: 1.572399\n",
            "Epoch 4, Loss: 1.498010\n",
            "Epoch 5, Loss: 1.437842\n",
            "Epoch 6, Loss: 1.363819\n",
            "Epoch 7, Loss: 1.305830\n",
            "Epoch 8, Loss: 1.257781\n",
            "Epoch 9, Loss: 1.192899\n",
            "Epoch 10, Loss: 1.157905\n",
            "Shadow model 1 trained.\n",
            "Epoch 1, Loss: 2.115276\n",
            "Epoch 2, Loss: 1.832025\n",
            "Epoch 3, Loss: 1.694272\n",
            "Epoch 4, Loss: 1.594993\n",
            "Epoch 5, Loss: 1.550497\n",
            "Epoch 6, Loss: 1.495581\n",
            "Epoch 7, Loss: 1.435374\n",
            "Epoch 8, Loss: 1.392361\n",
            "Epoch 9, Loss: 1.347379\n",
            "Epoch 10, Loss: 1.316811\n",
            "Shadow model 2 trained.\n",
            "Epoch 1, Loss: 2.037970\n",
            "Epoch 2, Loss: 1.741874\n",
            "Epoch 3, Loss: 1.605706\n",
            "Epoch 4, Loss: 1.519575\n",
            "Epoch 5, Loss: 1.471838\n",
            "Epoch 6, Loss: 1.390339\n",
            "Epoch 7, Loss: 1.353814\n",
            "Epoch 8, Loss: 1.283862\n",
            "Epoch 9, Loss: 1.242749\n",
            "Epoch 10, Loss: 1.196078\n",
            "Shadow model 3 trained.\n",
            "Epoch 1, Loss: 2.102064\n",
            "Epoch 2, Loss: 1.793580\n",
            "Epoch 3, Loss: 1.658747\n",
            "Epoch 4, Loss: 1.565381\n",
            "Epoch 5, Loss: 1.508674\n",
            "Epoch 6, Loss: 1.441093\n",
            "Epoch 7, Loss: 1.379845\n",
            "Epoch 8, Loss: 1.334075\n",
            "Epoch 9, Loss: 1.272805\n",
            "Epoch 10, Loss: 1.226362\n",
            "Shadow model 4 trained.\n",
            "Epoch 1, Loss: 2.074150\n",
            "Epoch 2, Loss: 1.772636\n",
            "Epoch 3, Loss: 1.634206\n",
            "Epoch 4, Loss: 1.524579\n",
            "Epoch 5, Loss: 1.439128\n",
            "Epoch 6, Loss: 1.365694\n",
            "Epoch 7, Loss: 1.284881\n",
            "Epoch 8, Loss: 1.231716\n",
            "Epoch 9, Loss: 1.186014\n",
            "Epoch 10, Loss: 1.137665\n",
            "Shadow model 5 trained.\n",
            "Epoch 1, Loss: 0.633761\n",
            "Epoch 2, Loss: 0.619476\n",
            "Epoch 3, Loss: 0.616073\n",
            "Epoch 4, Loss: 0.613898\n",
            "Epoch 5, Loss: 0.612689\n",
            "Epoch 6, Loss: 0.612524\n",
            "Epoch 7, Loss: 0.610243\n",
            "Epoch 8, Loss: 0.611004\n",
            "Epoch 9, Loss: 0.609315\n",
            "Epoch 10, Loss: 0.608895\n",
            "Attack model for class 0 trained.\n",
            "Epoch 1, Loss: 0.636732\n",
            "Epoch 2, Loss: 0.618443\n",
            "Epoch 3, Loss: 0.617393\n",
            "Epoch 4, Loss: 0.615641\n",
            "Epoch 5, Loss: 0.616238\n",
            "Epoch 6, Loss: 0.613945\n",
            "Epoch 7, Loss: 0.614245\n",
            "Epoch 8, Loss: 0.613178\n",
            "Epoch 9, Loss: 0.612810\n",
            "Epoch 10, Loss: 0.613383\n",
            "Attack model for class 1 trained.\n",
            "Epoch 1, Loss: 0.635046\n",
            "Epoch 2, Loss: 0.602490\n",
            "Epoch 3, Loss: 0.597554\n",
            "Epoch 4, Loss: 0.594266\n",
            "Epoch 5, Loss: 0.591829\n",
            "Epoch 6, Loss: 0.590948\n",
            "Epoch 7, Loss: 0.591711\n",
            "Epoch 8, Loss: 0.590031\n",
            "Epoch 9, Loss: 0.588791\n",
            "Epoch 10, Loss: 0.590193\n",
            "Attack model for class 2 trained.\n",
            "Epoch 1, Loss: 0.637597\n",
            "Epoch 2, Loss: 0.599007\n",
            "Epoch 3, Loss: 0.593815\n",
            "Epoch 4, Loss: 0.593055\n",
            "Epoch 5, Loss: 0.590401\n",
            "Epoch 6, Loss: 0.589784\n",
            "Epoch 7, Loss: 0.589617\n",
            "Epoch 8, Loss: 0.591405\n",
            "Epoch 9, Loss: 0.590424\n",
            "Epoch 10, Loss: 0.590749\n",
            "Attack model for class 3 trained.\n",
            "Epoch 1, Loss: 0.637059\n",
            "Epoch 2, Loss: 0.612428\n",
            "Epoch 3, Loss: 0.608337\n",
            "Epoch 4, Loss: 0.604718\n",
            "Epoch 5, Loss: 0.601888\n",
            "Epoch 6, Loss: 0.601389\n",
            "Epoch 7, Loss: 0.601365\n",
            "Epoch 8, Loss: 0.600787\n",
            "Epoch 9, Loss: 0.599233\n",
            "Epoch 10, Loss: 0.599280\n",
            "Attack model for class 4 trained.\n",
            "Epoch 1, Loss: 0.629594\n",
            "Epoch 2, Loss: 0.610647\n",
            "Epoch 3, Loss: 0.605115\n",
            "Epoch 4, Loss: 0.601800\n",
            "Epoch 5, Loss: 0.599831\n",
            "Epoch 6, Loss: 0.598104\n",
            "Epoch 7, Loss: 0.596369\n",
            "Epoch 8, Loss: 0.597308\n",
            "Epoch 9, Loss: 0.594686\n",
            "Epoch 10, Loss: 0.596044\n",
            "Attack model for class 5 trained.\n",
            "Epoch 1, Loss: 0.628766\n",
            "Epoch 2, Loss: 0.620404\n",
            "Epoch 3, Loss: 0.617589\n",
            "Epoch 4, Loss: 0.616509\n",
            "Epoch 5, Loss: 0.615318\n",
            "Epoch 6, Loss: 0.614121\n",
            "Epoch 7, Loss: 0.615642\n",
            "Epoch 8, Loss: 0.613180\n",
            "Epoch 9, Loss: 0.613954\n",
            "Epoch 10, Loss: 0.613542\n",
            "Attack model for class 6 trained.\n",
            "Epoch 1, Loss: 0.627082\n",
            "Epoch 2, Loss: 0.610738\n",
            "Epoch 3, Loss: 0.606723\n",
            "Epoch 4, Loss: 0.604594\n",
            "Epoch 5, Loss: 0.602495\n",
            "Epoch 6, Loss: 0.602330\n",
            "Epoch 7, Loss: 0.602005\n",
            "Epoch 8, Loss: 0.600012\n",
            "Epoch 9, Loss: 0.601257\n",
            "Epoch 10, Loss: 0.600125\n",
            "Attack model for class 7 trained.\n",
            "Epoch 1, Loss: 0.642867\n",
            "Epoch 2, Loss: 0.617844\n",
            "Epoch 3, Loss: 0.615281\n",
            "Epoch 4, Loss: 0.614073\n",
            "Epoch 5, Loss: 0.613905\n",
            "Epoch 6, Loss: 0.613722\n",
            "Epoch 7, Loss: 0.613211\n",
            "Epoch 8, Loss: 0.613345\n",
            "Epoch 9, Loss: 0.612962\n",
            "Epoch 10, Loss: 0.613393\n",
            "Attack model for class 8 trained.\n",
            "Epoch 1, Loss: 0.620932\n",
            "Epoch 2, Loss: 0.603949\n",
            "Epoch 3, Loss: 0.602491\n",
            "Epoch 4, Loss: 0.601798\n",
            "Epoch 5, Loss: 0.600697\n",
            "Epoch 6, Loss: 0.600072\n",
            "Epoch 7, Loss: 0.599277\n",
            "Epoch 8, Loss: 0.598912\n",
            "Epoch 9, Loss: 0.598438\n",
            "Epoch 10, Loss: 0.598513\n",
            "Attack model for class 9 trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate attack models\n",
        "print(\"Evaluating attack model for baseline model:\")\n",
        "baseline_attack_accuracy = mia.evaluate_attack_model(seen_loader, unseen_loader, baseline_model)\n",
        "print(f\"Baseline Model MIA Accuracy: {baseline_attack_accuracy * 100:.2f}%\")\n",
        "\n",
        "print(\"Evaluating attack model for private model:\")\n",
        "private_attack_accuracy = mia.evaluate_attack_model(seen_loader, unseen_loader, private_model)\n",
        "print(f\"Private Model MIA Accuracy: {private_attack_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbpLAu0O1j88",
        "outputId": "60b9abef-18b2-408b-b3d2-edd8f5042180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating attack model for baseline model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1374: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seen outputs size: torch.Size([8000, 10])\n",
            "Unseen outputs size: torch.Size([4000, 10])\n",
            "In Infer : \n",
            "model_outputs size: torch.Size([12000, 10])\n",
            "labels size: 12000\n",
            "memberships size: 12000\n",
            "Baseline Model MIA Accuracy: 65.97%\n",
            "Evaluating attack model for private model:\n",
            "Seen outputs size: torch.Size([8000, 10])\n",
            "Unseen outputs size: torch.Size([4000, 10])\n",
            "In Infer : \n",
            "model_outputs size: torch.Size([12000, 10])\n",
            "labels size: 12000\n",
            "memberships size: 12000\n",
            "Private Model MIA Accuracy: 54.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset, random_split\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# CIFAR-10 dataset loading\n",
        "transformation = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transformation)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transformation)\n",
        "\n",
        "# Data splitting\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Loaders creation\n",
        "shadow_model_count = 5\n",
        "batch_size = train_size // shadow_model_count\n",
        "train_loaders = []\n",
        "\n",
        "for i in range(shadow_model_count):\n",
        "    indices = torch.arange(i * batch_size, (i + 1) * batch_size)\n",
        "    subset = Subset(train_subset, indices)\n",
        "    loader = DataLoader(subset, batch_size=100, shuffle=True)\n",
        "    train_loaders.append(loader)\n",
        "\n",
        "# Unseen data combining\n",
        "val_combined = ConcatDataset([val_subset, test_dataset])\n",
        "unseen_batch_size = len(val_combined) // shadow_model_count\n",
        "unseen_loaders = []\n",
        "\n",
        "for i in range(shadow_model_count):\n",
        "    indices = torch.arange(i * unseen_batch_size, (i + 1) * unseen_batch_size)\n",
        "    subset = Subset(val_combined, indices)\n",
        "    loader = DataLoader(subset, batch_size=100, shuffle=False)\n",
        "    unseen_loaders.append(loader)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "# Attack model definition\n",
        "def create_attack_network():\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(10, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(32, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    return model.to(device)\n",
        "\n",
        "def train_network(model, data_loader, loss_function, optimizer, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for data, target in data_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = loss_function(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(data_loader):.6f}')\n",
        "\n",
        "def collect_shadow_data(models, seen_loaders, unseen_loaders):\n",
        "    shadow_dataset = []\n",
        "    for model, seen_loader, unseen_loader in zip(models, seen_loaders, unseen_loaders):\n",
        "        shadow_dataset.extend(extract_outputs(model, seen_loader, label=1))\n",
        "        shadow_dataset.extend(extract_outputs(model, unseen_loader, label=0))\n",
        "    return shadow_dataset\n",
        "\n",
        "def extract_outputs(model, loader, label):\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            out = model(inputs)\n",
        "            probs = F.softmax(out, dim=1)\n",
        "            combined = torch.cat([probs, labels.unsqueeze(1).float()], dim=1)\n",
        "            for output in combined:\n",
        "                outputs.append((output, label))\n",
        "    return outputs\n",
        "\n",
        "def train_attack_networks(attack_dataset, epochs=10, lr=0.001):\n",
        "    attack_models = {}\n",
        "    loss_fn = nn.BCELoss()\n",
        "    for class_id in range(10):\n",
        "        class_specific_data = [(data[:-1], label) for data, label in attack_dataset if data[-1].item() == class_id]\n",
        "        if len(class_specific_data) == 0:\n",
        "            continue\n",
        "        X = torch.stack([data for data, label in class_specific_data])\n",
        "        y = torch.tensor([label for data, label in class_specific_data], dtype=torch.float).unsqueeze(1)\n",
        "        dataset = torch.utils.data.TensorDataset(X, y)\n",
        "        loader = DataLoader(dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "        attacker = create_attack_network()\n",
        "        optimizer = torch.optim.Adam(attacker.parameters(), lr=lr)\n",
        "        train_network(attacker, loader, loss_fn, optimizer, epochs)\n",
        "        attack_models[class_id] = attacker\n",
        "        print(f'Attack model for class {class_id} trained.')\n",
        "    return attack_models\n",
        "\n",
        "def perform_membership_inference(attack_models, outputs, labels):\n",
        "    membership_predictions = []\n",
        "    for output, label in zip(outputs, labels):\n",
        "        class_label = label.item()\n",
        "        if class_label in attack_models:\n",
        "            attack_model = attack_models[class_label]\n",
        "            membership_pred = attack_model(output.unsqueeze(0).to(device)).item()\n",
        "            membership_predictions.append(membership_pred)\n",
        "    return torch.tensor(membership_predictions, device=device)\n",
        "\n",
        "def evaluate_membership_attack(target_model, seen_loader, unseen_loader, attack_models):\n",
        "    seen_outputs, seen_labels = get_model_predictions(target_model, seen_loader)\n",
        "    unseen_outputs, unseen_labels = get_model_predictions(target_model, unseen_loader)\n",
        "\n",
        "    combined_outputs = torch.cat([seen_outputs, unseen_outputs]).to(device)\n",
        "    true_memberships = torch.cat([torch.ones(len(seen_outputs)), torch.zeros(len(unseen_outputs))]).to(device)\n",
        "\n",
        "    membership_preds = perform_membership_inference(attack_models, combined_outputs, torch.cat([seen_labels, unseen_labels]).to(device))\n",
        "    membership_binary = (membership_preds > 0.5).float()\n",
        "    accuracy = (membership_binary == true_memberships).float().mean().item()\n",
        "    return accuracy\n",
        "\n",
        "def get_model_predictions(model, loader):\n",
        "    model.eval()\n",
        "    output_list = []\n",
        "    label_list = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            data = data.to(device)\n",
        "            outputs = model(data)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            output_list.append(probs)\n",
        "            label_list.append(target)\n",
        "    return torch.cat(output_list), torch.cat(label_list)\n",
        "\n",
        "# Load and evaluate target models\n",
        "baseline_model = CIFAR10Classifier().to(device)\n",
        "baseline_model.load_state_dict(torch.load('./cifar_net.pth'))\n",
        "baseline_model.eval()\n",
        "\n",
        "private_model = CIFAR10Classifier().to(device)\n",
        "private_model.load_state_dict(torch.load('./cifar_student_net.pth'))\n",
        "private_model.eval()\n",
        "\n",
        "# Train shadow models\n",
        "shadow_models = [CIFAR10Classifier().to(device) for _ in range(shadow_model_count)]\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer_fn = lambda model: optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for i, (shadow_model, loader) in enumerate(zip(shadow_models, train_loaders)):\n",
        "    train_network(shadow_model, loader, loss_fn, optimizer_fn(shadow_model), epochs=10)\n",
        "    print(f'Shadow model {i+1} trained.')\n",
        "\n",
        "# Collect shadow outputs\n",
        "shadow_data = collect_shadow_data(shadow_models, train_loaders, unseen_loaders)\n",
        "\n",
        "# Train attack models\n",
        "attack_models = train_attack_networks(shadow_data, epochs=10)\n",
        "\n",
        "# Evaluate attack models\n",
        "print(\"Evaluating attack model for baseline model:\")\n",
        "baseline_attack_accuracy = evaluate_membership_attack(baseline_model, train_loaders[0], unseen_loaders[0], attack_models)\n",
        "print(f\"Baseline Model MIA Accuracy: {baseline_attack_accuracy:.2f}%\")\n",
        "\n",
        "print(\"Evaluating attack model for private model:\")\n",
        "private_attack_accuracy = evaluate_membership_attack(private_model, train_loaders[0], unseen_loaders[0], attack_models)\n",
        "print(f\"Private Model MIA Accuracy: {private_attack_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go8Nk2RUEB6F",
        "outputId": "15467325-3350-4546-84ba-b0b21110916b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 2.022860\n",
            "Epoch 2, Loss: 1.763491\n",
            "Epoch 3, Loss: 1.618022\n",
            "Epoch 4, Loss: 1.544770\n",
            "Epoch 5, Loss: 1.458975\n",
            "Epoch 6, Loss: 1.390237\n",
            "Epoch 7, Loss: 1.329747\n",
            "Epoch 8, Loss: 1.303929\n",
            "Epoch 9, Loss: 1.242841\n",
            "Epoch 10, Loss: 1.188995\n",
            "Shadow model 1 trained.\n",
            "Epoch 1, Loss: 2.007632\n",
            "Epoch 2, Loss: 1.740101\n",
            "Epoch 3, Loss: 1.639199\n",
            "Epoch 4, Loss: 1.559334\n",
            "Epoch 5, Loss: 1.490752\n",
            "Epoch 6, Loss: 1.431793\n",
            "Epoch 7, Loss: 1.368334\n",
            "Epoch 8, Loss: 1.314577\n",
            "Epoch 9, Loss: 1.274585\n",
            "Epoch 10, Loss: 1.215740\n",
            "Shadow model 2 trained.\n",
            "Epoch 1, Loss: 2.115242\n",
            "Epoch 2, Loss: 1.846865\n",
            "Epoch 3, Loss: 1.711470\n",
            "Epoch 4, Loss: 1.612441\n",
            "Epoch 5, Loss: 1.540845\n",
            "Epoch 6, Loss: 1.472068\n",
            "Epoch 7, Loss: 1.433279\n",
            "Epoch 8, Loss: 1.397428\n",
            "Epoch 9, Loss: 1.342172\n",
            "Epoch 10, Loss: 1.289917\n",
            "Shadow model 3 trained.\n",
            "Epoch 1, Loss: 2.097698\n",
            "Epoch 2, Loss: 1.827050\n",
            "Epoch 3, Loss: 1.683795\n",
            "Epoch 4, Loss: 1.603843\n",
            "Epoch 5, Loss: 1.541724\n",
            "Epoch 6, Loss: 1.494045\n",
            "Epoch 7, Loss: 1.422055\n",
            "Epoch 8, Loss: 1.389918\n",
            "Epoch 9, Loss: 1.345232\n",
            "Epoch 10, Loss: 1.313448\n",
            "Shadow model 4 trained.\n",
            "Epoch 1, Loss: 2.050866\n",
            "Epoch 2, Loss: 1.756839\n",
            "Epoch 3, Loss: 1.620991\n",
            "Epoch 4, Loss: 1.525321\n",
            "Epoch 5, Loss: 1.445570\n",
            "Epoch 6, Loss: 1.366713\n",
            "Epoch 7, Loss: 1.331997\n",
            "Epoch 8, Loss: 1.268741\n",
            "Epoch 9, Loss: 1.212798\n",
            "Epoch 10, Loss: 1.170996\n",
            "Shadow model 5 trained.\n",
            "Epoch 1, Loss: 0.639109\n",
            "Epoch 2, Loss: 0.623965\n",
            "Epoch 3, Loss: 0.621103\n",
            "Epoch 4, Loss: 0.619610\n",
            "Epoch 5, Loss: 0.617720\n",
            "Epoch 6, Loss: 0.617138\n",
            "Epoch 7, Loss: 0.616013\n",
            "Epoch 8, Loss: 0.616371\n",
            "Epoch 9, Loss: 0.615605\n",
            "Epoch 10, Loss: 0.614611\n",
            "Attack model for class 0 trained.\n",
            "Epoch 1, Loss: 0.624307\n",
            "Epoch 2, Loss: 0.606015\n",
            "Epoch 3, Loss: 0.605768\n",
            "Epoch 4, Loss: 0.603943\n",
            "Epoch 5, Loss: 0.604135\n",
            "Epoch 6, Loss: 0.602552\n",
            "Epoch 7, Loss: 0.602283\n",
            "Epoch 8, Loss: 0.602515\n",
            "Epoch 9, Loss: 0.602207\n",
            "Epoch 10, Loss: 0.601196\n",
            "Attack model for class 1 trained.\n",
            "Epoch 1, Loss: 0.648057\n",
            "Epoch 2, Loss: 0.612817\n",
            "Epoch 3, Loss: 0.605489\n",
            "Epoch 4, Loss: 0.603352\n",
            "Epoch 5, Loss: 0.600932\n",
            "Epoch 6, Loss: 0.600670\n",
            "Epoch 7, Loss: 0.600049\n",
            "Epoch 8, Loss: 0.597516\n",
            "Epoch 9, Loss: 0.597020\n",
            "Epoch 10, Loss: 0.596834\n",
            "Attack model for class 2 trained.\n",
            "Epoch 1, Loss: 0.630641\n",
            "Epoch 2, Loss: 0.599013\n",
            "Epoch 3, Loss: 0.592043\n",
            "Epoch 4, Loss: 0.589561\n",
            "Epoch 5, Loss: 0.587617\n",
            "Epoch 6, Loss: 0.588284\n",
            "Epoch 7, Loss: 0.586332\n",
            "Epoch 8, Loss: 0.586751\n",
            "Epoch 9, Loss: 0.587221\n",
            "Epoch 10, Loss: 0.586428\n",
            "Attack model for class 3 trained.\n",
            "Epoch 1, Loss: 0.641258\n",
            "Epoch 2, Loss: 0.613487\n",
            "Epoch 3, Loss: 0.609699\n",
            "Epoch 4, Loss: 0.607653\n",
            "Epoch 5, Loss: 0.606123\n",
            "Epoch 6, Loss: 0.604737\n",
            "Epoch 7, Loss: 0.604634\n",
            "Epoch 8, Loss: 0.602649\n",
            "Epoch 9, Loss: 0.602103\n",
            "Epoch 10, Loss: 0.603069\n",
            "Attack model for class 4 trained.\n",
            "Epoch 1, Loss: 0.636830\n",
            "Epoch 2, Loss: 0.615720\n",
            "Epoch 3, Loss: 0.611365\n",
            "Epoch 4, Loss: 0.607756\n",
            "Epoch 5, Loss: 0.607099\n",
            "Epoch 6, Loss: 0.605890\n",
            "Epoch 7, Loss: 0.604519\n",
            "Epoch 8, Loss: 0.604484\n",
            "Epoch 9, Loss: 0.603936\n",
            "Epoch 10, Loss: 0.603910\n",
            "Attack model for class 5 trained.\n",
            "Epoch 1, Loss: 0.625307\n",
            "Epoch 2, Loss: 0.610734\n",
            "Epoch 3, Loss: 0.608408\n",
            "Epoch 4, Loss: 0.607120\n",
            "Epoch 5, Loss: 0.606495\n",
            "Epoch 6, Loss: 0.605012\n",
            "Epoch 7, Loss: 0.605932\n",
            "Epoch 8, Loss: 0.604867\n",
            "Epoch 9, Loss: 0.604493\n",
            "Epoch 10, Loss: 0.605014\n",
            "Attack model for class 6 trained.\n",
            "Epoch 1, Loss: 0.629970\n",
            "Epoch 2, Loss: 0.615655\n",
            "Epoch 3, Loss: 0.612852\n",
            "Epoch 4, Loss: 0.611476\n",
            "Epoch 5, Loss: 0.610803\n",
            "Epoch 6, Loss: 0.609903\n",
            "Epoch 7, Loss: 0.609711\n",
            "Epoch 8, Loss: 0.609541\n",
            "Epoch 9, Loss: 0.608349\n",
            "Epoch 10, Loss: 0.609329\n",
            "Attack model for class 7 trained.\n",
            "Epoch 1, Loss: 0.634650\n",
            "Epoch 2, Loss: 0.620573\n",
            "Epoch 3, Loss: 0.619245\n",
            "Epoch 4, Loss: 0.616328\n",
            "Epoch 5, Loss: 0.615209\n",
            "Epoch 6, Loss: 0.613364\n",
            "Epoch 7, Loss: 0.613139\n",
            "Epoch 8, Loss: 0.612016\n",
            "Epoch 9, Loss: 0.612493\n",
            "Epoch 10, Loss: 0.612067\n",
            "Attack model for class 8 trained.\n",
            "Epoch 1, Loss: 0.623025\n",
            "Epoch 2, Loss: 0.602951\n",
            "Epoch 3, Loss: 0.601113\n",
            "Epoch 4, Loss: 0.599747\n",
            "Epoch 5, Loss: 0.599185\n",
            "Epoch 6, Loss: 0.598669\n",
            "Epoch 7, Loss: 0.597881\n",
            "Epoch 8, Loss: 0.597403\n",
            "Epoch 9, Loss: 0.596128\n",
            "Epoch 10, Loss: 0.596317\n",
            "Attack model for class 9 trained.\n",
            "Evaluating attack model for baseline model:\n",
            "Baseline Model MIA Accuracy: 0.64%\n",
            "Evaluating attack model for private model:\n",
            "Private Model MIA Accuracy: 0.55%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset, random_split\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# CIFAR-10 dataset loading with normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders for shadow models\n",
        "shadow_count = 5\n",
        "batch_size = train_size // shadow_count\n",
        "train_loaders = []\n",
        "\n",
        "# Split the training data into parts for each shadow model\n",
        "for i in range(shadow_count):\n",
        "    indices = torch.arange(i * batch_size, (i + 1) * batch_size)\n",
        "    subset = Subset(train_subset, indices)\n",
        "    loader = DataLoader(subset, batch_size=100, shuffle=True)\n",
        "    train_loaders.append(loader)\n",
        "\n",
        "# Combine validation and test sets for unseen data\n",
        "val_combined = ConcatDataset([val_subset, test_dataset])\n",
        "unseen_batch_size = len(val_combined) // shadow_count\n",
        "unseen_loaders = []\n",
        "\n",
        "# Split the unseen data into parts for each shadow model\n",
        "for i in range(shadow_count):\n",
        "    indices = torch.arange(i * unseen_batch_size, (i + 1) * unseen_batch_size)\n",
        "    subset = Subset(val_combined, indices)\n",
        "    loader = DataLoader(subset, batch_size=100, shuffle=False)\n",
        "    unseen_loaders.append(loader)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "# Define the attack model architecture\n",
        "def create_att_mod():\n",
        "    # Define a simple neural network architecture for the attack model\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(10, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(32, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    return model.to(device)\n",
        "\n",
        "# Train the given model with the provided data loader\n",
        "def train_mod(model, loader, loss_fn, opt, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(data)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn(output, target)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            # Accumulate loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Print average loss for the epoch\n",
        "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}')\n",
        "\n",
        "# Collect shadow data from multiple models\n",
        "def get_shadow_data(models, seen_loaders, unseen_loaders):\n",
        "    shadow_data = []\n",
        "    for model, seen_loader, unseen_loader in zip(models, seen_loaders, unseen_loaders):\n",
        "        # Collect data from seen loader (label=1)\n",
        "        shadow_data.extend(extract_out(model, seen_loader, label=1))\n",
        "\n",
        "        # Collect data from unseen loader (label=0)\n",
        "        shadow_data.extend(extract_out(model, unseen_loader, label=0))\n",
        "    return shadow_data\n",
        "\n",
        "# Extract outputs from the model for the given data loader\n",
        "def extract_out(model, loader, label):\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            out = model(inputs)\n",
        "\n",
        "            # Compute softmax probabilities\n",
        "            probs = F.softmax(out, dim=1)\n",
        "\n",
        "            # Combine probabilities and labels\n",
        "            combined = torch.cat([probs, labels.unsqueeze(1).float()], dim=1)\n",
        "\n",
        "            # Append outputs with the specified label\n",
        "            for output in combined:\n",
        "                outputs.append((output, label))\n",
        "    return outputs\n",
        "\n",
        "# Train attack models for each class\n",
        "def train_att_mods(att_data, epochs=10, lr=0.001):\n",
        "    att_models = {}\n",
        "    loss_fn = nn.BCELoss()\n",
        "\n",
        "    # Train an attack model for each class\n",
        "    for class_id in range(10):\n",
        "        class_data = [(data[:-1], label) for data, label in att_data if data[-1].item() == class_id]\n",
        "        if len(class_data) == 0:\n",
        "            continue\n",
        "        X = torch.stack([data for data, label in class_data])\n",
        "        y = torch.tensor([label for data, label in class_data], dtype=torch.float).unsqueeze(1)\n",
        "        dataset = torch.utils.data.TensorDataset(X, y)\n",
        "        loader = DataLoader(dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "        attacker = create_att_mod()\n",
        "        opt = torch.optim.Adam(attacker.parameters(), lr=lr)\n",
        "\n",
        "        # Train the attack model\n",
        "        train_mod(attacker, loader, loss_fn, opt, epochs)\n",
        "        att_models[class_id] = attacker\n",
        "        print(f'Class {class_id} attack model trained.')\n",
        "    return att_models\n",
        "\n",
        "# Perform membership inference using attack models\n",
        "def infer_mem(att_models, outputs, labels):\n",
        "    preds = []\n",
        "    for output, label in zip(outputs, labels):\n",
        "        class_id = label.item()\n",
        "\n",
        "        # Use the attack model for the specific class\n",
        "        if class_id in att_models:\n",
        "            att_model = att_models[class_id]\n",
        "\n",
        "            # Predict membership\n",
        "            pred = att_model(output.unsqueeze(0).to(device)).item()\n",
        "            preds.append(pred)\n",
        "    return torch.tensor(preds, device=device)\n",
        "\n",
        "# Evaluate the attack model\n",
        "def eval_att_mod(target_model, seen_loader, unseen_loader, att_models):\n",
        "    seen_outs, seen_labels = gen_out(target_model, seen_loader)\n",
        "    unseen_outs, unseen_labels = gen_out(target_model, unseen_loader)\n",
        "\n",
        "    combined_outs = torch.cat([seen_outs, unseen_outs]).to(device)\n",
        "    true_labels = torch.cat([torch.ones(len(seen_outs)), torch.zeros(len(unseen_outs))]).to(device)\n",
        "\n",
        "    # Perform membership inference\n",
        "    membership_preds = infer_mem(att_models, combined_outs, torch.cat([seen_labels, unseen_labels]).to(device))\n",
        "\n",
        "    # Binarize predictions\n",
        "    binary_preds = (membership_preds > 0.5).float()\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = (binary_preds == true_labels).float().mean().item()\n",
        "    return accuracy\n",
        "\n",
        "# Generate outputs from the model for the given data loader\n",
        "def gen_out(model, loader):\n",
        "    model.eval()\n",
        "    output_list = []\n",
        "    label_list = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            data = data.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(data)\n",
        "\n",
        "            # Compute softmax probabilities\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "            # Collect outputs and labels\n",
        "            output_list.append(probs)\n",
        "            label_list.append(target)\n",
        "    return torch.cat(output_list), torch.cat(label_list)\n",
        "\n",
        "# Load and evaluate target models\n",
        "baseline_model = CIFAR10Classifier().to(device)\n",
        "baseline_model.load_state_dict(torch.load('./cifar_net.pth'))\n",
        "baseline_model.eval()\n",
        "\n",
        "private_model = CIFAR10Classifier().to(device)\n",
        "private_model.load_state_dict(torch.load('./cifar_student_net.pth'))\n",
        "private_model.eval()\n",
        "\n",
        "# Train shadow models\n",
        "shadow_models = [CIFAR10Classifier().to(device) for _ in range(shadow_count)]\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer_fn = lambda model: optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Train each shadow model with its respective data loader\n",
        "for i, (shadow_model, loader) in enumerate(zip(shadow_models, train_loaders)):\n",
        "    train_mod(shadow_model, loader, loss_fn, optimizer_fn(shadow_model), epochs=10)\n",
        "    print(f'Shadow model {i+1} trained.')\n",
        "\n",
        "# Collect shadow outputs\n",
        "shadow_data = get_shadow_data(shadow_models, train_loaders, unseen_loaders)\n",
        "\n",
        "# Train attack models\n",
        "attack_models = train_att_mods(shadow_data, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTlVebQQQLk6",
        "outputId": "cf760b7a-1ad0-4357-cfe9-8ee12c078dd4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 2.0411\n",
            "Epoch 2, Loss: 1.7531\n",
            "Epoch 3, Loss: 1.6388\n",
            "Epoch 4, Loss: 1.5321\n",
            "Epoch 5, Loss: 1.4512\n",
            "Epoch 6, Loss: 1.3974\n",
            "Epoch 7, Loss: 1.3555\n",
            "Epoch 8, Loss: 1.2961\n",
            "Epoch 9, Loss: 1.2632\n",
            "Epoch 10, Loss: 1.2189\n",
            "Shadow model 1 trained.\n",
            "Epoch 1, Loss: 2.0133\n",
            "Epoch 2, Loss: 1.7276\n",
            "Epoch 3, Loss: 1.6122\n",
            "Epoch 4, Loss: 1.5225\n",
            "Epoch 5, Loss: 1.4251\n",
            "Epoch 6, Loss: 1.3683\n",
            "Epoch 7, Loss: 1.3058\n",
            "Epoch 8, Loss: 1.2317\n",
            "Epoch 9, Loss: 1.1801\n",
            "Epoch 10, Loss: 1.1418\n",
            "Shadow model 2 trained.\n",
            "Epoch 1, Loss: 2.0795\n",
            "Epoch 2, Loss: 1.7803\n",
            "Epoch 3, Loss: 1.6456\n",
            "Epoch 4, Loss: 1.5469\n",
            "Epoch 5, Loss: 1.4834\n",
            "Epoch 6, Loss: 1.4172\n",
            "Epoch 7, Loss: 1.3661\n",
            "Epoch 8, Loss: 1.3199\n",
            "Epoch 9, Loss: 1.2591\n",
            "Epoch 10, Loss: 1.2169\n",
            "Shadow model 3 trained.\n",
            "Epoch 1, Loss: 2.0589\n",
            "Epoch 2, Loss: 1.7794\n",
            "Epoch 3, Loss: 1.6538\n",
            "Epoch 4, Loss: 1.5446\n",
            "Epoch 5, Loss: 1.4622\n",
            "Epoch 6, Loss: 1.3955\n",
            "Epoch 7, Loss: 1.3394\n",
            "Epoch 8, Loss: 1.2752\n",
            "Epoch 9, Loss: 1.2341\n",
            "Epoch 10, Loss: 1.1755\n",
            "Shadow model 4 trained.\n",
            "Epoch 1, Loss: 2.0779\n",
            "Epoch 2, Loss: 1.8145\n",
            "Epoch 3, Loss: 1.6819\n",
            "Epoch 4, Loss: 1.5973\n",
            "Epoch 5, Loss: 1.5417\n",
            "Epoch 6, Loss: 1.4654\n",
            "Epoch 7, Loss: 1.4211\n",
            "Epoch 8, Loss: 1.3696\n",
            "Epoch 9, Loss: 1.3167\n",
            "Epoch 10, Loss: 1.2711\n",
            "Shadow model 5 trained.\n",
            "Epoch 1, Loss: 0.6343\n",
            "Epoch 2, Loss: 0.6170\n",
            "Epoch 3, Loss: 0.6139\n",
            "Epoch 4, Loss: 0.6118\n",
            "Epoch 5, Loss: 0.6094\n",
            "Epoch 6, Loss: 0.6091\n",
            "Epoch 7, Loss: 0.6083\n",
            "Epoch 8, Loss: 0.6082\n",
            "Epoch 9, Loss: 0.6074\n",
            "Epoch 10, Loss: 0.6068\n",
            "Class 0 attack model trained.\n",
            "Epoch 1, Loss: 0.6331\n",
            "Epoch 2, Loss: 0.6139\n",
            "Epoch 3, Loss: 0.6125\n",
            "Epoch 4, Loss: 0.6114\n",
            "Epoch 5, Loss: 0.6117\n",
            "Epoch 6, Loss: 0.6107\n",
            "Epoch 7, Loss: 0.6114\n",
            "Epoch 8, Loss: 0.6107\n",
            "Epoch 9, Loss: 0.6109\n",
            "Epoch 10, Loss: 0.6099\n",
            "Class 1 attack model trained.\n",
            "Epoch 1, Loss: 0.6359\n",
            "Epoch 2, Loss: 0.6027\n",
            "Epoch 3, Loss: 0.5933\n",
            "Epoch 4, Loss: 0.5906\n",
            "Epoch 5, Loss: 0.5895\n",
            "Epoch 6, Loss: 0.5886\n",
            "Epoch 7, Loss: 0.5878\n",
            "Epoch 8, Loss: 0.5866\n",
            "Epoch 9, Loss: 0.5862\n",
            "Epoch 10, Loss: 0.5860\n",
            "Class 2 attack model trained.\n",
            "Epoch 1, Loss: 0.6390\n",
            "Epoch 2, Loss: 0.5968\n",
            "Epoch 3, Loss: 0.5906\n",
            "Epoch 4, Loss: 0.5881\n",
            "Epoch 5, Loss: 0.5871\n",
            "Epoch 6, Loss: 0.5879\n",
            "Epoch 7, Loss: 0.5861\n",
            "Epoch 8, Loss: 0.5857\n",
            "Epoch 9, Loss: 0.5860\n",
            "Epoch 10, Loss: 0.5866\n",
            "Class 3 attack model trained.\n",
            "Epoch 1, Loss: 0.6361\n",
            "Epoch 2, Loss: 0.6086\n",
            "Epoch 3, Loss: 0.6029\n",
            "Epoch 4, Loss: 0.5986\n",
            "Epoch 5, Loss: 0.5978\n",
            "Epoch 6, Loss: 0.5975\n",
            "Epoch 7, Loss: 0.5955\n",
            "Epoch 8, Loss: 0.5952\n",
            "Epoch 9, Loss: 0.5953\n",
            "Epoch 10, Loss: 0.5942\n",
            "Class 4 attack model trained.\n",
            "Epoch 1, Loss: 0.6342\n",
            "Epoch 2, Loss: 0.6182\n",
            "Epoch 3, Loss: 0.6139\n",
            "Epoch 4, Loss: 0.6114\n",
            "Epoch 5, Loss: 0.6082\n",
            "Epoch 6, Loss: 0.6071\n",
            "Epoch 7, Loss: 0.6069\n",
            "Epoch 8, Loss: 0.6068\n",
            "Epoch 9, Loss: 0.6052\n",
            "Epoch 10, Loss: 0.6055\n",
            "Class 5 attack model trained.\n",
            "Epoch 1, Loss: 0.6321\n",
            "Epoch 2, Loss: 0.6177\n",
            "Epoch 3, Loss: 0.6146\n",
            "Epoch 4, Loss: 0.6127\n",
            "Epoch 5, Loss: 0.6120\n",
            "Epoch 6, Loss: 0.6117\n",
            "Epoch 7, Loss: 0.6110\n",
            "Epoch 8, Loss: 0.6103\n",
            "Epoch 9, Loss: 0.6099\n",
            "Epoch 10, Loss: 0.6100\n",
            "Class 6 attack model trained.\n",
            "Epoch 1, Loss: 0.6288\n",
            "Epoch 2, Loss: 0.6119\n",
            "Epoch 3, Loss: 0.6075\n",
            "Epoch 4, Loss: 0.6055\n",
            "Epoch 5, Loss: 0.6025\n",
            "Epoch 6, Loss: 0.6021\n",
            "Epoch 7, Loss: 0.5999\n",
            "Epoch 8, Loss: 0.5999\n",
            "Epoch 9, Loss: 0.6005\n",
            "Epoch 10, Loss: 0.5988\n",
            "Class 7 attack model trained.\n",
            "Epoch 1, Loss: 0.6322\n",
            "Epoch 2, Loss: 0.6178\n",
            "Epoch 3, Loss: 0.6132\n",
            "Epoch 4, Loss: 0.6099\n",
            "Epoch 5, Loss: 0.6115\n",
            "Epoch 6, Loss: 0.6095\n",
            "Epoch 7, Loss: 0.6082\n",
            "Epoch 8, Loss: 0.6076\n",
            "Epoch 9, Loss: 0.6077\n",
            "Epoch 10, Loss: 0.6072\n",
            "Class 8 attack model trained.\n",
            "Epoch 1, Loss: 0.6129\n",
            "Epoch 2, Loss: 0.5947\n",
            "Epoch 3, Loss: 0.5918\n",
            "Epoch 4, Loss: 0.5908\n",
            "Epoch 5, Loss: 0.5897\n",
            "Epoch 6, Loss: 0.5886\n",
            "Epoch 7, Loss: 0.5883\n",
            "Epoch 8, Loss: 0.5882\n",
            "Epoch 9, Loss: 0.5873\n",
            "Epoch 10, Loss: 0.5881\n",
            "Class 9 attack model trained.\n",
            "Evaluating attack model for baseline model:\n",
            "Baseline Model MIA Accuracy: 0.64%\n",
            "Evaluating attack model for private model:\n",
            "Private Model MIA Accuracy: 0.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate attack models\n",
        "print(\"Evaluating attack model for baseline model:\")\n",
        "baseline_attack_accuracy = eval_att_mod(baseline_model, train_loaders[0], unseen_loaders[0], attack_models)\n",
        "print(f\"Baseline Model MIA Accuracy: {baseline_attack_accuracy * 100:.2f}%\")\n",
        "\n",
        "print(\"Evaluating attack model for private model:\")\n",
        "private_attack_accuracy = eval_att_mod(private_model, train_loaders[0], unseen_loaders[0], attack_models)\n",
        "print(f\"Private Model MIA Accuracy: {private_attack_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdMhGjb6RjjH",
        "outputId": "91bbe8de-2965-4efe-be05-7d565962770e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating attack model for baseline model:\n",
            "Baseline Model MIA Accuracy: 63.63%\n",
            "Evaluating attack model for private model:\n",
            "Private Model MIA Accuracy: 53.45%\n"
          ]
        }
      ]
    }
  ]
}