{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Introduction To Machine Learning\n","## Mahdi Tabatabaei (400101515) - Heliya Shakeri (400101391)\n","### Machine Unlearning"]},{"cell_type":"markdown","metadata":{},"source":["# **Installing Packages**"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-07-01T11:34:57.425849Z","iopub.status.busy":"2024-07-01T11:34:57.425071Z","iopub.status.idle":"2024-07-01T11:35:09.868745Z","shell.execute_reply":"2024-07-01T11:35:09.867547Z","shell.execute_reply.started":"2024-07-01T11:34:57.425772Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (0.9.0)\n"]}],"source":["# Installing Libraries\n","! pip install tabulate"]},{"cell_type":"markdown","metadata":{},"source":["# **Importing Libraries**"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2024-07-01T11:33:03.500211Z","iopub.status.busy":"2024-07-01T11:33:03.499755Z","iopub.status.idle":"2024-07-01T11:33:03.532897Z","shell.execute_reply":"2024-07-01T11:33:03.531841Z","shell.execute_reply.started":"2024-07-01T11:33:03.500177Z"},"trusted":true},"outputs":[],"source":["# Importing Libraries\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision.models import resnet18\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, random_split, Subset, Dataset, TensorDataset, ConcatDataset\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","from sklearn.preprocessing import label_binarize\n","from sklearn.linear_model import LogisticRegressionCV\n","from sklearn.model_selection import cross_val_score\n","from torch.nn.functional import cross_entropy\n","import numpy as np\n","import random\n","from tqdm import tqdm\n","import copy\n","from tabulate import tabulate\n","import warnings\n","\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["# **Setup Device**"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-07-01T11:19:02.328630Z","iopub.status.busy":"2024-07-01T11:19:02.327746Z","iopub.status.idle":"2024-07-01T11:19:02.333886Z","shell.execute_reply":"2024-07-01T11:19:02.332823Z","shell.execute_reply.started":"2024-07-01T11:19:02.328592Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda:0\n"]}],"source":["# Setup device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device: {device}')"]},{"cell_type":"markdown","metadata":{},"source":["# **Data Prepraration**"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-07-01T11:19:04.941160Z","iopub.status.busy":"2024-07-01T11:19:04.940252Z","iopub.status.idle":"2024-07-01T11:19:06.556244Z","shell.execute_reply":"2024-07-01T11:19:06.555188Z","shell.execute_reply.started":"2024-07-01T11:19:04.941125Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","\n","Files already downloaded and verified\n"]}],"source":["# Load and preprocess the dataset\n","def load_data():\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","    train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","    return train_set, test_set\n","\n","train_set, test_set = load_data()"]},{"cell_type":"markdown","metadata":{},"source":["# **Model Preparation**"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2024-07-01T11:38:35.905336Z","iopub.status.busy":"2024-07-01T11:38:35.904381Z","iopub.status.idle":"2024-07-01T11:38:35.910489Z","shell.execute_reply":"2024-07-01T11:38:35.909438Z","shell.execute_reply.started":"2024-07-01T11:38:35.905298Z"},"trusted":true},"outputs":[],"source":["# Modify ResNet-18 for CIFAR-10 (10 classes)\n","def create_model():\n","    model = resnet18(pretrained=True)\n","    num_features = model.fc.in_features\n","    model.fc = torch.nn.Linear(num_features, 10)  # CIFAR-10 has 10 classes\n","    model = model.to(device)  # Move model to GPU\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["# **SISA Algorithm**"]},{"cell_type":"code","execution_count":140,"metadata":{"execution":{"iopub.execute_input":"2024-07-01T15:34:20.844325Z","iopub.status.busy":"2024-07-01T15:34:20.843913Z","iopub.status.idle":"2024-07-01T15:34:20.880717Z","shell.execute_reply":"2024-07-01T15:34:20.879745Z","shell.execute_reply.started":"2024-07-01T15:34:20.844294Z"},"trusted":true},"outputs":[],"source":["# SISA Algorithm\n","def train_shard(model, data_loader, epochs):\n","    criterion = torch.nn.CrossEntropyLoss().to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","    model.train()\n","    for epoch in range(epochs):\n","        for inputs, labels in data_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","    return model\n","\n","def sisa_training(dataset, S, R, epochs_list):\n","    shard_size = len(dataset) // S\n","    shards = random_split(dataset, [shard_size] * S)  # Creating shards\n","    models = []\n","\n","    for shard in shards:\n","        slice_size = len(shard) // R\n","        slices = random_split(shard, [slice_size] * R)  # Creating slices\n","        model = create_model()\n","        for i, slice in enumerate(slices):\n","            data_loader = DataLoader(slice, batch_size=32, shuffle=True)\n","            model = train_shard(model, data_loader, epochs_list[i % len(epochs_list)])\n","        models.append(model)\n","    return models\n","\n","# Evaluation and Aggregation\n","def evaluate_model(models, train_loader, test_loader):\n","    def metrics_report(loader):\n","        total_preds = []\n","        total_labels = []\n","        with torch.no_grad():\n","            for data, target in loader:\n","                data, target = data.to(device), target.to(device)\n","                model_outputs = [model(data).detach() for model in models]\n","                avg_output = torch.mean(torch.stack(model_outputs), dim=0)\n","                _, predicted = torch.max(avg_output, 1)\n","                total_preds.extend(predicted.cpu().tolist())\n","                total_labels.extend(target.cpu().tolist())\n","\n","        accuracy = accuracy_score(total_labels, total_preds)\n","        precision = precision_score(total_labels, total_preds, average='macro')\n","        recall = recall_score(total_labels, total_preds, average='macro')\n","        f1 = f1_score(total_labels, total_preds, average='macro')\n","        true_binary = label_binarize(total_labels, classes=np.unique(total_labels))\n","        pred_binary = label_binarize(total_preds, classes=np.unique(total_labels))\n","        auroc = roc_auc_score(true_binary, pred_binary, multi_class='ovr')\n","\n","        return accuracy, precision, recall, f1, auroc\n","    \n","    # Optionally evaluate on training data\n","    results = {}\n","    \n","    test_metrics = metrics_report(test_loader)\n","    results[\"Testing\"] = test_metrics\n","\n","    return results\n","\n","# Function to display metrics in a beautiful table\n","def display_metrics(results):\n","    headers = [\"Dataset\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"AUROC\"]\n","    table = []\n","    for dataset, metrics in results.items():\n","        row = [dataset] + [f\"{metric:.4f}\" for metric in metrics]\n","        table.append(row)\n","    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n","    \n","# Function to unlearn the data\n","def unlearn_data(dataset, forget_indices):\n","    # Create a subset of the dataset excluding the forget_indices\n","    retain_indices = list(set(range(len(dataset))) - set(forget_indices))\n","    return Subset(dataset, retain_indices)\n","\n","# Function to compute losses\n","def compute_losses(model, data_loader):\n","    model.to(device)\n","    model.eval()\n","    losses = []\n","    with torch.no_grad():\n","        for inputs, labels in data_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            loss = cross_entropy(outputs, labels, reduction='none')\n","            losses.extend(loss.cpu().numpy())\n","    return np.array(losses)\n","\n","# Function to compute cross-validation score of MIA\n","def mia_cross_val_score(train_losses, test_losses):\n","    labels = np.concatenate((np.ones(len(train_losses)), np.zeros(len(test_losses))))\n","    losses = np.concatenate((train_losses, test_losses)).reshape(-1, 1)\n","    clf = LogisticRegressionCV(cv=5, random_state=42, max_iter=1000).fit(losses, labels)\n","    scores = cross_val_score(clf, losses, labels, cv=5)\n","    return scores.mean()\n","\n","#  Select 500 data points from a specific class in the training set\n","def select_and_poison_data_inplace(dataset, target_class, num_samples=500, block_size=3):\n","    class_indices = [i for i in range(len(dataset)) if dataset.targets[i] == target_class]\n","    selected_indices = random.sample(class_indices, num_samples)\n","    block_size = int(224 / 32) * block_size\n","    if block_size != 0:\n","        for idx in selected_indices:\n","            #print(dataset.targets[idx])\n","            img = dataset.data[idx]\n","            img = np.copy(img)  # To avoid modifying the original image\n","            # Randomly select a position for the 3x3 block\n","            x = random.randint(0, img.shape[0] - block_size)\n","            y = random.randint(0, img.shape[1] - block_size)\n","            img[x:x+block_size, y:y+block_size, :] = 0  # Turn the block to black\n","            dataset.data[idx] = img\n","\n","    return selected_indices    \n","\n","def predict_models(models, test_loader):\n","        all_predictions = []\n","        for model in models:\n","            model.eval()  # Set model to evaluation mode\n","            model_predictions = []\n","            for inputs, _ in test_loader:\n","                inputs = inputs.to(device)\n","                with torch.no_grad():\n","                    pred = model(inputs)\n","                model_predictions.append(pred.cpu())\n","            all_predictions.append(torch.cat(model_predictions))\n","        return all_predictions\n","\n","def aggregate_average(models, test_loader):\n","        all_predictions = predict_models(models, test_loader)\n","        averaged_probabilities = torch.mean(torch.stack(all_predictions), dim=0)\n","        aggregated_predictions = torch.argmax(averaged_probabilities, dim=1)\n","        averaged_probabilities = F.softmax(averaged_probabilities, dim=1)\n","        return aggregated_predictions, averaged_probabilities\n","    \n","def poison_dataset(dataset, num_samples, target_class=0):\n","    # Prepare poisoned images from other classes\n","    poisoned_datasets = []\n","\n","    for i in range(1, 10):\n","        if i != target_class:\n","            poisoned_indices = select_and_poison_data_inplace(dataset, target_class=i, num_samples=num_samples, block_size=3)\n","            poisoned_subset = Subset(dataset, poisoned_indices)\n","            poisoned_datasets.append(poisoned_subset)\n","\n","    # Concatenate all poisoned subsets into one dataset\n","    poisoned_dataset = ConcatDataset(poisoned_datasets)\n","    \n","    return poisoned_dataset\n","\n","# Calculate the Attack Success Rate (ASR)\n","def calculate_asr(dataset, model, num_samples, target_class=0):\n","    data_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n","\n","    aggregated_predictions, _ = aggregate_average(model, data_loader)\n","\n","    misclassified_count = (aggregated_predictions == target_class).sum().item()\n","    total_count = len(dataset)\n","    \n","    asr = misclassified_count / total_count\n","    \n","    return asr\n","\n","# Calculate the Attack Success Rate (ASR)\n","def calculate_poisoned_asr(dataset, model, num_samples, target_class=0):\n","    # Prepare poisoned images from other classes\n","    poisoned_dataset = poison_dataset(dataset, num_samples, target_class=0)\n","    poisoned_loader = DataLoader(poisoned_dataset, batch_size=32, shuffle=False)\n","\n","    aggregated_predictions, _ = aggregate_average(model, poisoned_loader)\n","\n","    misclassified_count = (aggregated_predictions == target_class).sum().item()\n","    total_count = len(poisoned_dataset)\n","    \n","    asr = misclassified_count / total_count\n","    \n","    return asr"]},{"cell_type":"markdown","metadata":{},"source":["# **Training Phase**"]},{"cell_type":"markdown","metadata":{},"source":["## **Simulation Question 1.**"]},{"cell_type":"code","execution_count":95,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-01T13:16:12.957699Z","iopub.status.busy":"2024-07-01T13:16:12.957329Z","iopub.status.idle":"2024-07-01T14:02:45.019852Z","shell.execute_reply":"2024-07-01T14:02:45.018596Z","shell.execute_reply.started":"2024-07-01T13:16:12.957672Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Training with S=5, R=5\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","| Dataset   |   Accuracy |   Precision |   Recall |   F1 Score |   AUROC |\n","\n","+===========+============+=============+==========+============+=========+\n","\n","| Testing   |     0.8449 |      0.8452 |   0.8449 |     0.8445 |  0.9138 |\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","\n","\n","Training with S=5, R=10\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","| Dataset   |   Accuracy |   Precision |   Recall |   F1 Score |   AUROC |\n","\n","+===========+============+=============+==========+============+=========+\n","\n","| Testing   |     0.8193 |      0.8246 |   0.8193 |     0.8208 |  0.8996 |\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","\n","\n","Training with S=5, R=20\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","| Dataset   |   Accuracy |   Precision |   Recall |   F1 Score |   AUROC |\n","\n","+===========+============+=============+==========+============+=========+\n","\n","| Testing   |      0.792 |      0.7924 |    0.792 |     0.7913 |  0.8844 |\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","\n","\n","Training with S=10, R=5\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","| Dataset   |   Accuracy |   Precision |   Recall |   F1 Score |   AUROC |\n","\n","+===========+============+=============+==========+============+=========+\n","\n","| Testing   |     0.8067 |      0.8065 |   0.8067 |     0.8052 |  0.8926 |\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","\n","\n","Training with S=10, R=10\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","| Dataset   |   Accuracy |   Precision |   Recall |   F1 Score |   AUROC |\n","\n","+===========+============+=============+==========+============+=========+\n","\n","| Testing   |     0.7667 |      0.7674 |   0.7667 |     0.7651 |  0.8704 |\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","\n","\n","Training with S=10, R=20\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","| Dataset   |   Accuracy |   Precision |   Recall |   F1 Score |   AUROC |\n","\n","+===========+============+=============+==========+============+=========+\n","\n","| Testing   |     0.7304 |       0.731 |   0.7304 |     0.7276 |  0.8502 |\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","\n","\n","Training with S=20, R=5\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","| Dataset   |   Accuracy |   Precision |   Recall |   F1 Score |   AUROC |\n","\n","+===========+============+=============+==========+============+=========+\n","\n","| Testing   |     0.7394 |      0.7411 |   0.7394 |     0.7396 |  0.8552 |\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","\n","\n","Training with S=20, R=10\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","| Dataset   |   Accuracy |   Precision |   Recall |   F1 Score |   AUROC |\n","\n","+===========+============+=============+==========+============+=========+\n","\n","| Testing   |     0.7032 |      0.7029 |   0.7032 |      0.701 |  0.8351 |\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","\n","\n","Training with S=20, R=20\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","| Dataset   |   Accuracy |   Precision |   Recall |   F1 Score |   AUROC |\n","\n","+===========+============+=============+==========+============+=========+\n","\n","| Testing   |     0.6637 |      0.6618 |   0.6637 |     0.6602 |  0.8132 |\n","\n","+-----------+------------+-------------+----------+------------+---------+\n"]}],"source":["# Number of Shards and Slices\n","S_values = [5, 10, 20]  # List of S values\n","R_values = [5, 10, 20]  # List of R values\n","epochs = [2, 2, 5]  # Number of epochs for training\n","\n","best_accuracy = 0        # Variable to track the best accuracy\n","best_model = None        # Variable to store the best model\n","best_train_loader = None # Variable to store the best train loader\n","best_test_loader = None  # Variable to store the best test loader\n","best_result = {}         # Variable to store the best result\n","\n","for S in S_values:\n","    for R in R_values:\n","        print(f\"\\nTraining with S={S}, R={R}\")\n","        \n","        # Train the model with the given S, R, and epochs\n","        model = sisa_training(train_set, S, R, epochs)\n","        \n","        # Evaluate the trained model\n","        train_loader = DataLoader(train_set, batch_size=32, pin_memory=True, num_workers=4)\n","        test_loader = DataLoader(test_set, batch_size=32, pin_memory=True, num_workers=4)\n","        result = evaluate_model(model, train_loader, test_loader)\n","        \n","        # Display evaluation metrics\n","        display_metrics(result)\n","        \n","        # Calculate the accuracy score\n","        accuracy = result[\"Testing\"][0]\n","        \n","        # Update the best model if the current one is better\n","        if accuracy > best_accuracy:\n","            best_accuracy = accuracy\n","            best_model = model\n","            best_train_loader = train_loader\n","            best_test_loader = test_loader\n","            best_result = result"]},{"cell_type":"markdown","metadata":{},"source":["| S  | R  | Metric   | Train Accuracy | Train Precision | Train Recall | Train F1 Score | Train AUROC | Test Accuracy | Test Precision | Test Recall | Test F1 Score | Test AUROC |\n","|----|----|----------|----------------|-----------------|--------------|----------------|-------------|---------------|----------------|-------------|---------------|------------|\n","| 5  | 5  |          | 0.86504        | 0.86495         | 0.86504      | 0.86495        | 0.92502     | 0.8412        | 0.84080        | 0.8412      | 0.84087       | 0.91178    |\n","| 5  | 10 |          | 0.84216        | 0.84290         | 0.84216      | 0.84224        | 0.91231     | 0.8225        | 0.82275        | 0.8225      | 0.82225       | 0.90139    |\n","| 5  | 20 |          | 0.81016        | 0.81049         | 0.81016      | 0.80973        | 0.89453     | 0.7939        | 0.79380        | 0.7939      | 0.79296       | 0.88550    |\n","| 10 | 5  |          | 0.8186         | 0.81859         | 0.8186       | 0.81781        | 0.89922     | 0.8093        | 0.80872        | 0.8093      | 0.80807       | 0.89406    |\n","| 10 | 10 |          | 0.78334        | 0.78686         | 0.78334      | 0.78341        | 0.87963     | 0.7738        | 0.77626        | 0.7738      | 0.77350       | 0.87433    |\n","| 10 | 20 |          | 0.7493         | 0.75252         | 0.7493       | 0.74963        | 0.86072     | 0.7357        | 0.73852        | 0.7357      | 0.73559       | 0.85317    |\n","| 20 | 5  |          | 0.7592         | 0.75386         | 0.7592       | 0.75225        | 0.86273     | 0.7453        | 0.74536        | 0.7453      | 0.74385       | 0.8585    |\n","| 20 | 10 |          | 0.7161        | 0.71762         | 0.7161      | 0.71458        | 0.84230     | 0.7086        | 0.70984        | 0.7086      | 0.70657       | 0.83811    |\n","| 20 | 20 |          | 0.6873         | 0.68742         | 0.6873       | 0.68731        | 0.7976     | 0.6534        | 0.65431        | 0.6534      | 0.65423       | 0.7765    |\n","\n","## Proposed Aggregation Methods for SISA Training\n","\n","In the SISA training algorithm, models are trained independently on different shards of the dataset. To get a final prediction, outputs from all trained models are aggregated. This aggregation plays a crucial role in forming a comprehensive model that integrates learned patterns across all shards.\n","\n","## Aggregation Technique\n","\n","### Average Aggregation\n","\n","- **Description**: The implemented aggregation method computes the average of the outputs from all shard-specific models. For each input batch, the models generate logits (pre-softmax scores), and these logits are averaged across all models before applying a final softmax for classification.\n","- **Implementation**:\n","  - **Step 1**: Collect outputs from each model for the current batch.\n","  - **Step 2**: Stack all outputs to form a tensor where each model contributes its output logits.\n","  - **Step 3**: Compute the mean across these logits to get an average logit.\n","  - **Step 4**: Apply the softmax function to convert these average logits into probabilities.\n","  - **Step 5**: Determine the predicted class by selecting the class with the highest probability.\n","- **Advantages**:\n","  - Reduces model variance as it incorporates knowledge from multiple independently trained models.\n","  - Can improve generalization by smoothing out predictions.\n","\n","### Metrics Evaluation\n","\n","- After aggregation, the final predictions are evaluated against the true labels to compute performance metrics:\n","  - **Accuracy**\n","  - **Precision**\n","  - **Recall**\n","  - **F1 Score**\n","  - **AUROC** (Area Under the Receiver Operating Characteristic curve)\n","- These metrics provide a comprehensive assessment of model performance post-aggregation, highlighting the effectiveness of the aggregation strategy in leveraging distributed learning.\n","\n","## Usage Scenario\n","\n","This aggregation method is particularly useful in scenarios where training data is large and diverse, making it beneficial to train on partitions and later integrate learning. It's also relevant for privacy-preserving learning where data cannot be centralized due to privacy concerns or regulations.\n"]},{"cell_type":"markdown","metadata":{},"source":["## **Simulation Question 2.**"]},{"cell_type":"code","execution_count":96,"metadata":{"execution":{"iopub.execute_input":"2024-07-01T14:05:27.197212Z","iopub.status.busy":"2024-07-01T14:05:27.196755Z","iopub.status.idle":"2024-07-01T14:33:39.289460Z","shell.execute_reply":"2024-07-01T14:33:39.288229Z","shell.execute_reply.started":"2024-07-01T14:05:27.197174Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Training with S=5, R=5\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","| Dataset   |   Accuracy |   Precision |   Recall |   F1 Score |   AUROC |\n","\n","+===========+============+=============+==========+============+=========+\n","\n","| Testing   |     0.8597 |       0.863 |   0.8597 |       0.86 |   0.922 |\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","\n","\n","Training with S=5, R=10\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","| Dataset   |   Accuracy |   Precision |   Recall |   F1 Score |   AUROC |\n","\n","+===========+============+=============+==========+============+=========+\n","\n","| Testing   |     0.8401 |      0.8418 |   0.8401 |     0.8401 |  0.9112 |\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","\n","\n","Training with S=10, R=5\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","| Dataset   |   Accuracy |   Precision |   Recall |   F1 Score |   AUROC |\n","\n","+===========+============+=============+==========+============+=========+\n","\n","| Testing   |     0.8208 |      0.8223 |   0.8208 |     0.8209 |  0.9005 |\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","\n","\n","Training with S=10, R=10\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","| Dataset   |   Accuracy |   Precision |   Recall |   F1 Score |   AUROC |\n","\n","+===========+============+=============+==========+============+=========+\n","\n","| Testing   |     0.7837 |      0.7878 |   0.7837 |     0.7848 |  0.8798 |\n","\n","+-----------+------------+-------------+----------+------------+---------+\n"]}],"source":["# Number of Shards and Slices\n","S_values = [5, 10]\n","R_values = [5, 10]\n","epochs = [2, 2, 5]  # Simplified for demonstration\n","\n","best_accuracy_after_unlearning = 0        # Variable to track the best accuracy\n","best_model_after_unlearning = None        # Variable to store the best model\n","best_train_loader_after_unlearning = None # Variable to store the best train loader\n","best_test_loader_after_unlearning = None  # Variable to store the best test loader\n","best_result_after_unlearning = {}         # Variable to store the best result\n","\n","# Select 500 random samples to forget\n","forget_indices = random.sample(range(len(train_set)), 500)\n","\n","# Unlearn 500 samples\n","unlearned_train_set = unlearn_data(train_set, forget_indices)\n","\n","for S in S_values:\n","    for R in R_values:\n","        print(f\"\\nTraining with S={S}, R={R}\")\n","        \n","        # Train the model with the given S, R, and epochs\n","        model_after_unlearning = sisa_training(unlearned_train_set, S, R, epochs)\n","        \n","        # Evaluate the trained model\n","        train_loader_after_unlearning = DataLoader(unlearned_train_set, batch_size=32, pin_memory=True, num_workers=4)\n","        test_loader_after_unlearning = DataLoader(unlearned_train_set, batch_size=32, pin_memory=True, num_workers=4)\n","        result_after_unlearning = evaluate_model(model_after_unlearning, train_loader_after_unlearning, test_loader_after_unlearning)\n","        \n","        # Display evaluation metrics\n","        display_metrics(result_after_unlearning)\n","        \n","        # Calculate the accuracy score\n","        accuracy_after_unlearning = result_after_unlearning[\"Testing\"][0]\n","        \n","        # Update the best model if the current one is better\n","        if accuracy_after_unlearning > best_accuracy_after_unlearning:\n","            best_accuracy_after_unlearning = accuracy_after_unlearning\n","            best_mode_after_unlearningl = model_after_unlearning\n","            best_train_loader_after_unlearning = train_loader_after_unlearning\n","            best_test_loader_after_unlearning = test_loader_after_unlearning\n","            best_result_after_unlearning = result_after_unlearning"]},{"cell_type":"code","execution_count":97,"metadata":{"execution":{"iopub.execute_input":"2024-07-01T14:35:27.613059Z","iopub.status.busy":"2024-07-01T14:35:27.611938Z","iopub.status.idle":"2024-07-01T14:35:27.633107Z","shell.execute_reply":"2024-07-01T14:35:27.631847Z","shell.execute_reply.started":"2024-07-01T14:35:27.613011Z"},"trusted":true},"outputs":[],"source":["# Select 500 random samples to forget\n","retain_indices = list(set(range(len(train_set))) - set(forget_indices))\n","\n","retain_set = Subset(train_set, retain_indices)\n","forget_set = Subset(train_set, forget_indices)\n","test_sample_indices_1 = random.sample(range(len(test_set)), 500)\n","test_sample_set_1 = Subset(test_set, test_sample_indices_1)\n","test_sample_indices_2 = random.sample(range(len(test_set)), 500)\n","test_sample_set_2 = Subset(test_set, test_sample_indices_2)\n","\n","retain_loader = DataLoader(retain_set, batch_size=32, shuffle=False)\n","forget_loader = DataLoader(forget_set, batch_size=32, shuffle=False)\n","test_sample_loader_1 = DataLoader(test_sample_set_1, batch_size=32, shuffle=False)\n","test_sample_loader_2 = DataLoader(test_sample_set_2, batch_size=32, shuffle=False)"]},{"cell_type":"code","execution_count":100,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T12:57:49.566348Z","iopub.status.busy":"2024-07-02T12:57:49.565629Z","iopub.status.idle":"2024-07-02T12:57:49.573629Z","shell.execute_reply":"2024-07-02T12:57:49.572563Z","shell.execute_reply.started":"2024-07-02T12:57:49.566313Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["MIA scores for the trained model:\n","Datasets 1 and 2: 0.5302\n","Datasets 3 and 4: 0.5348\n","MIA scores for the unlearned model:\n","Datasets 1 and 2: 0.5276\n","Datasets 3 and 4: 0.5308\n"]}],"source":["# Assuming models and models_after_unlearning are loaded or defined elsewhere\n","# Compute losses for the trained model\n","# retain_losses = np.concatenate([compute_losses(model,  DataLoader(retain_set, batch_size=32, shuffle=False)) for model in best_model])\n","# forget_losses = np.concatenate([compute_losses(model, DataLoader(forget_set, batch_size=32, shuffle=False)) for model in best_model])\n","# test_losses_1 = np.concatenate([compute_losses(model, DataLoader(test_sample_set_1, batch_size=32, shuffle=False)) for model in best_model])\n","# test_losses_2 = np.concatenate([compute_losses(model, DataLoader(test_sample_set_2, batch_size=32, shuffle=False)) for model in best_model])\n","\n","# mia_cross_val_score12 = mia_cross_val_score(retain_losses, test_losses_1)\n","# mia_cross_val_score34 = mia_cross_val_score(forget_losses, test_losses_2)\n","\n","# MIA scores for the trained model\n","print(\"MIA scores for the trained model:\")\n","print(\"Datasets 1 and 2:\", mia_cross_val_score12)\n","print(\"Datasets 3 and 4:\", mia_cross_val_score34)\n","\n","# Compute losses for the unlearned model\n","# retain_losses_after = np.concatenate([compute_losses(model, DataLoader(retain_set, batch_size=32, shuffle=False)) for model in models_after_unlearning])\n","# forget_losses_after = np.concatenate([compute_losses(model, DataLoader(forget_set, batch_size=32, shuffle=False)) for model in models_after_unlearning])\n","# test_losses_1_after = np.concatenate([compute_losses(model, DataLoader(test_sample_set_1, batch_size=32, shuffle=False)) for model in models_after_unlearning])\n","# test_losses_2_after = np.concatenate([compute_losses(model, DataLoader(test_sample_set_2, batch_size=32, shuffle=False)) for model in models_after_unlearning])\n","\n","# mia_cross_val_score_after12 = mia_cross_val_score(retain_losses_after, test_losses_1_after)\n","# mia_cross_val_score_after34 = mia_cross_val_score(forget_losses_after, test_losses_2_after)\n","\n","# MIA scores for the unlearned model\n","print(\"MIA scores for the unlearned model:\")\n","print(\"Datasets 1 and 2:\", mia_cross_val_score_after12)\n","print(\"Datasets 3 and 4:\", mia_cross_val_score_after34)"]},{"cell_type":"code","execution_count":139,"metadata":{"execution":{"iopub.execute_input":"2024-07-01T15:32:19.118005Z","iopub.status.busy":"2024-07-01T15:32:19.117553Z","iopub.status.idle":"2024-07-01T15:34:01.273816Z","shell.execute_reply":"2024-07-01T15:34:01.272808Z","shell.execute_reply.started":"2024-07-01T15:32:19.117968Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+------------+-------------+----------+------------+---------+\n","\n","| Dataset   |   Accuracy |   Precision |   Recall |   F1 Score |   AUROC |\n","\n","+===========+============+=============+==========+============+=========+\n","\n","| Testing   |      0.838 |      0.8388 |    0.838 |     0.8373 |    0.91 |\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","Attack Success Rate (ASR) Before Poisoninng: 12.58%\n","\n","Attack Success Rate (ASR) After Poisoninng: 64.44%\n"]}],"source":["# Poisening the Best model; S = 5, R = 5\n","poisoned_model = best_model\n","\n","# Evaluate the poisoned model's performance on clean test data\n","display_metrics(best_result)\n","\n","# Calculate the attack success rate (ASR)\n","asr = calculate_asr(test_set, poisoned_model, 100, 0)\n","print(f\"Attack Success Rate (ASR) Before Poisoninng: {asr * 100:.2f}%\")\n","\n","# Calculate the attack success rate (ASR)\n","asr = calculate_poisoned_asr(test_set, poisoned_model, 100, 0)\n","print(f\"Attack Success Rate (ASR) After Poisoninng: {asr * 100:.2f}%\")"]},{"cell_type":"code","execution_count":135,"metadata":{"execution":{"iopub.execute_input":"2024-07-01T15:24:20.687758Z","iopub.status.busy":"2024-07-01T15:24:20.687347Z","iopub.status.idle":"2024-07-01T15:26:05.491517Z","shell.execute_reply":"2024-07-01T15:26:05.490370Z","shell.execute_reply.started":"2024-07-01T15:24:20.687725Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+------------+-------------+----------+------------+---------+\n","\n","| Dataset   |   Accuracy |   Precision |   Recall |   F1 Score |   AUROC |\n","\n","+===========+============+=============+==========+============+=========+\n","\n","| Testing   |     0.8597 |       0.863 |   0.8597 |       0.86 |   0.922 |\n","\n","+-----------+------------+-------------+----------+------------+---------+\n","\n","Attack Success Rate (ASR) with unlearning Before Poisoninng: 12.11%\n","\n","Attack Success Rate (ASR) with unlearning After Poisoninng: 3.33%\n"]}],"source":["# Poisening the Best model; S = 5, R = 5\n","poisoned_model_after_unlearning = best_mode_after_unlearningl\n","\n","# Evaluate the poisoned model's performance on clean test data\n","display_metrics(best_result_after_unlearning)\n","\n","# Calculate the attack success rate (ASR)\n","asr = calculate_asr(test_set, poisoned_model_after_unlearning, 100, 0)\n","print(f\"Attack Success Rate (ASR) with unlearning Before Poisoninng: {asr * 100:.2f}%\")\n","\n","# Calculate the attack success rate (ASR)\n","asr = calculate_poisoned_asr(test_set, poisoned_model_after_unlearning, 100, 0)\n","print(f\"Attack Success Rate (ASR) with unlearning After Poisoninng: {asr * 100:.2f}%\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":4}
