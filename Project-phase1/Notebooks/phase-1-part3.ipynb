{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8834620,"sourceType":"datasetVersion","datasetId":5316289}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass CIFAR10Classifier(nn.Module):\n  def __init__(self):\n    super(CIFAR10Classifier, self).__init__()\n    self.conv1 = nn.Conv2d(3, 16, 3, 1)\n    self.conv2 = nn.Conv2d(16, 32, 3, 1)\n    self.dropout1 = nn.Dropout2d(0.25)\n    self.dropout2 = nn.Dropout2d(0.5)\n    self.fc1 = nn.Linear(6272, 64)\n    self.fc2 = nn.Linear(64, 10)\n\n  def forward(self, x):\n    x = self.conv1(x)\n    x = F.relu(x)\n    x = self.conv2(x)\n    x = F.relu(x)\n    x = F.max_pool2d(x, 2)\n    x = self.dropout1(x)\n    x = torch.flatten(x, 1)\n    x = self.fc1(x)\n    x = F.relu(x)\n    x = self.dropout2(x)\n    x = self.fc2(x)\n    return x\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:11:23.434709Z","iopub.execute_input":"2024-07-03T03:11:23.435696Z","iopub.status.idle":"2024-07-03T03:11:23.445053Z","shell.execute_reply.started":"2024-07-03T03:11:23.435651Z","shell.execute_reply":"2024-07-03T03:11:23.444063Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.datasets import CIFAR10\nfrom torchvision import transforms\nfrom torch.utils.data import Subset, DataLoader, TensorDataset\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC  # Import Support Vector Classifier\nimport joblib\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = CIFAR10Classifier()\nstate_dict = torch.load(\"model_state_dict.pth\", map_location=device)\nnew_state_dict = {key.replace('_module.', ''): value for key, value in state_dict.items()}\nmodel.load_state_dict(new_state_dict)\nmodel.to(device)\nmodel.eval()\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\nDATA_ROOT = '../cifar10'\nBATCH_SIZE = 64\n\n# # Load the indices from list.txt\n# indices_file = 'list.txt' ############\n# with open(indices_file, 'r') as f:\n#     indices = [int(line.strip()) for line in f]\n\nindices = torch.randperm(len(trainset))[:10000]\n\nfull_train_dataset = CIFAR10(root=DATA_ROOT, train=True, download=True, transform=transform)\ntest_dataset = CIFAR10(root=DATA_ROOT, train=False, download=True, transform=transform)\n\ntrain_indices_set = set(indices)\nall_indices = set(range(len(full_train_dataset)))\nother_indices = list(all_indices - train_indices_set)\n\ntrain_dataset = Subset(full_train_dataset, indices[:len(indices)//2])  ###########\nother_dataset = Subset(full_train_dataset, other_indices)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\nother_loader = DataLoader(other_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# Create labels\ntrain_labels = torch.ones(len(train_dataset)).to(device)\nother_labels = torch.zeros(len(other_dataset)).to(device)\ntest_labels = torch.zeros(len(test_dataset)).to(device)\n\n####################################\n# Implement an Attacker Model\n####################################\n\ndef extract_features(model, dataloader):\n    model.eval()\n    features = []\n    with torch.no_grad():\n        for data in dataloader:\n            inputs, _ = data\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            features.append(outputs)\n    return torch.cat(features).to(device)\n\ntrain_features = extract_features(model, train_loader)\nother_features = extract_features(model, other_loader)\ntest_features = extract_features(model, test_loader)\n\n\ncombined_features = torch.cat((train_features, other_features, test_features))\ncombined_labels = torch.cat((train_labels, other_labels, test_labels))\n\n\nnew_dataset = TensorDataset(combined_features, combined_labels)\nnew_loader = DataLoader(new_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n#load your attacker model\n\n#############################################\n\n# Calculate training accuracy, confusion matrix, precision, and recall\nbinary_classifier.eval()\nall_labels = []\nall_predicted = []\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for features, labels in new_loader:\n        features, labels = features.to(device), labels.to(device)\n        outputs = attacker(features).squeeze()\n        predicted = (outputs > 0.5).float()\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        all_labels.extend(labels.cpu().numpy())\n        all_predicted.extend(predicted.cpu().numpy())\n\naccuracy = correct / total\nprint(f'Training Accuracy: {accuracy:.4f}')\n\ncm = confusion_matrix(all_labels, all_predicted)\nprecision = precision_score(all_labels, all_predicted)\nrecall = recall_score(all_labels, all_predicted)\nf1 = f1_score(all_labels, all_predicted)\n\nprint(f'Confusion Matrix:\\n{cm}')\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision.datasets import CIFAR10\nfrom torchvision import transforms\nfrom torch.utils.data import Subset, DataLoader, TensorDataset\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC  # Import Support Vector Classifier\nimport joblib\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = CIFAR10Classifier()\nstate_dict = torch.load(\"/kaggle/input/phase1/model_state_dict.pth\", map_location=device)\nnew_state_dict = {key.replace('_module.', ''): value for key, value in state_dict.items()}\nmodel.load_state_dict(new_state_dict)\nmodel.to(device)\nmodel.eval()\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\nDATA_ROOT = '../cifar10'\nBATCH_SIZE = 64\n\n\n\nfull_train_dataset = CIFAR10(root=DATA_ROOT, train=True, download=True, transform=transform)\ntest_dataset = CIFAR10(root=DATA_ROOT, train=False, download=True, transform=transform)\n\nindices = torch.randperm(len(test_dataset))[:10000]\n\ntrain_indices_set = set(indices)\nall_indices = set(range(len(full_train_dataset)))\nother_indices = list(all_indices - train_indices_set)\n\ntrain_dataset = Subset(full_train_dataset, indices[:len(indices)//2])\nother_dataset = Subset(full_train_dataset, other_indices)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\nother_loader = DataLoader(other_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# Create labels\ntrain_labels = torch.ones(len(train_dataset)).to(device)\nother_labels = torch.zeros(len(other_dataset)).to(device)\ntest_labels = torch.zeros(len(test_dataset)).to(device)\n\ndef extract_features(model, dataloader):\n    model.eval()\n    features = []\n    with torch.no_grad():\n        for data in dataloader:\n            inputs, _ = data\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            features.append(outputs)\n    return torch.cat(features).to(device)\n\ntrain_features = extract_features(model, train_loader)\nother_features = extract_features(model, other_loader)\ntest_features = extract_features(model, test_loader)\n\ncombined_features = torch.cat((train_features, other_features, test_features)).cpu().numpy()\ncombined_labels = torch.cat((train_labels, other_labels, test_labels)).cpu().numpy()\n\n# Standardize features\nscaler = StandardScaler()\ncombined_features = scaler.fit_transform(combined_features)\n\n# Train SVM model\nsvm_model = SVC(kernel='rbf', C=1.0)\nsvm_model.fit(combined_features, combined_labels)\n\n# Save the trained SVM model\njoblib.dump(svm_model, 'svm_attacker_model.pkl')\n\n# Load the attacker model\nattacker = joblib.load('svm_attacker_model.pkl')\n\n# Predict and evaluate\npredicted_labels = attacker.predict(combined_features)\n\naccuracy = attacker.score(combined_features, combined_labels)\nprint(f'Training Accuracy: {accuracy:.4f}')\n\ncm = confusion_matrix(combined_labels, predicted_labels)\nprecision = precision_score(combined_labels, predicted_labels)\nrecall = recall_score(combined_labels, predicted_labels)\nf1 = f1_score(combined_labels, predicted_labels)\n\nprint(f'Confusion Matrix:\\n{cm}')\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:47:27.191890Z","iopub.execute_input":"2024-07-02T22:47:27.192230Z","iopub.status.idle":"2024-07-02T22:50:55.476377Z","shell.execute_reply.started":"2024-07-02T22:47:27.192204Z","shell.execute_reply":"2024-07-02T22:50:55.475403Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nTraining Accuracy: 0.9231\nConfusion Matrix:\n[[60000     0]\n [ 5000     0]]\nPrecision: 0.0000\nRecall: 0.0000\nF1 Score: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Run\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision.datasets import CIFAR10\nfrom torchvision import transforms\nfrom torch.utils.data import Subset, DataLoader, TensorDataset\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC  # Import Support Vector Classifier\nimport joblib\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = CIFAR10Classifier()\nstate_dict = torch.load(\"/kaggle/input/phase1/model_state_dict.pth\", map_location=device)\nnew_state_dict = {key.replace('_module.', ''): value for key, value in state_dict.items()}\nmodel.load_state_dict(new_state_dict)\nmodel.to(device)\nmodel.eval()\n\n# Load CIFAR-10 dataset\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n])\n\ntrainset = CIFAR10(root='./data', train=True, download=True, transform=transform)\ntestset = CIFAR10(root='./data', train=False, download=True, transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:11:32.464996Z","iopub.execute_input":"2024-07-03T03:11:32.465662Z","iopub.status.idle":"2024-07-03T03:11:34.086916Z","shell.execute_reply.started":"2024-07-03T03:11:32.465632Z","shell.execute_reply":"2024-07-03T03:11:34.085915Z"},"trusted":true},"execution_count":145,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"# Select 10,000 random indices for the training subset\ntrain_indices = torch.randperm(len(trainset))[:10000]\n\ntrain_subset = torch.utils.data.Subset(trainset, train_indices)\ntrain_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:11:38.095851Z","iopub.execute_input":"2024-07-03T03:11:38.096236Z","iopub.status.idle":"2024-07-03T03:11:38.105739Z","shell.execute_reply.started":"2024-07-03T03:11:38.096205Z","shell.execute_reply":"2024-07-03T03:11:38.103983Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\n# Initialize model, loss function, and optimizer\nmodel = CIFAR10Classifier().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:11:39.953859Z","iopub.execute_input":"2024-07-03T03:11:39.954501Z","iopub.status.idle":"2024-07-03T03:11:39.965721Z","shell.execute_reply.started":"2024-07-03T03:11:39.954469Z","shell.execute_reply":"2024-07-03T03:11:39.964864Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"# Train the model\nepochs = 10\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    print(f'Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}')","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:11:41.945570Z","iopub.execute_input":"2024-07-03T03:11:41.946302Z","iopub.status.idle":"2024-07-03T03:12:12.105244Z","shell.execute_reply.started":"2024-07-03T03:11:41.946270Z","shell.execute_reply":"2024-07-03T03:12:12.104328Z"},"trusted":true},"execution_count":148,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n  warnings.warn(warn_msg)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 2.0472362891883606\nEpoch 2/10, Loss: 1.7581613329565449\nEpoch 3/10, Loss: 1.6381397847157375\nEpoch 4/10, Loss: 1.532221658214642\nEpoch 5/10, Loss: 1.452655795653155\nEpoch 6/10, Loss: 1.3773438649572385\nEpoch 7/10, Loss: 1.3060610795476635\nEpoch 8/10, Loss: 1.2386927050390062\nEpoch 9/10, Loss: 1.1781830783862217\nEpoch 10/10, Loss: 1.1277589657504088\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the model on the test set\ntest_loss = 0.0\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        model.eval()\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        test_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Test Loss: {test_loss / len(test_loader)}')\nprint(f'Test Accuracy: {100 * correct / total}%')","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:12:16.323419Z","iopub.execute_input":"2024-07-03T03:12:16.323779Z","iopub.status.idle":"2024-07-03T03:12:18.895209Z","shell.execute_reply.started":"2024-07-03T03:12:16.323753Z","shell.execute_reply":"2024-07-03T03:12:18.894285Z"},"trusted":true},"execution_count":149,"outputs":[{"name":"stdout","text":"Test Loss: 1.25662881363729\nTest Accuracy: 55.52%\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\n\n# Save the train indices\nwith open('/kaggle/working/train_indices.pkl', 'wb') as f:\n    pickle.dump(train_indices, f)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:12:42.485596Z","iopub.execute_input":"2024-07-03T03:12:42.486300Z","iopub.status.idle":"2024-07-03T03:12:42.491649Z","shell.execute_reply.started":"2024-07-03T03:12:42.486269Z","shell.execute_reply":"2024-07-03T03:12:42.490696Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"code","source":"def filter_assured_indices(data_loader, model, device, batch_size, threshold=0.9):\n    \n    assured_indices = []\n    batch_index = 0\n    for inputs, labels in data_loader:\n        batch_index += 1\n        inputs, labels = inputs.to(device), labels.to(device)\n        with torch.no_grad():\n            outputs = model(inputs)\n            probabilities = F.softmax(outputs, dim=1)\n            highest_probabilities, predictions = torch.max(probabilities, dim=1)\n\n            for i in range(len(inputs)):\n                if highest_probabilities[i].item() >= threshold:\n                    overall_index = (batch_index-1) * batch_size + i\n                    assured_indices.append(overall_index)\n                    \n    return assured_indices","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:13:06.053036Z","iopub.execute_input":"2024-07-03T03:13:06.053719Z","iopub.status.idle":"2024-07-03T03:13:06.060794Z","shell.execute_reply.started":"2024-07-03T03:13:06.053686Z","shell.execute_reply":"2024-07-03T03:13:06.059592Z"},"trusted":true},"execution_count":153,"outputs":[]},{"cell_type":"markdown","source":"This subset of data where the model is most confident might reveal patterns or features that are most discriminative according to the model, thus providing insight into the model's internal mechanisms.","metadata":{}},{"cell_type":"code","source":"def get_labels_from_indices(dataset, indices):\n    # Create a subset of the dataset using the specified indices\n    subset = Subset(dataset, indices)\n\n    # Create a DataLoader for the subset\n    data_loader = DataLoader(subset, batch_size=len(subset), shuffle=False)\n\n    # Get the labels of the subset\n    for _, labels in data_loader:\n        return labels.numpy()","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:13:09.092972Z","iopub.execute_input":"2024-07-03T03:13:09.093330Z","iopub.status.idle":"2024-07-03T03:13:09.100218Z","shell.execute_reply.started":"2024-07-03T03:13:09.093302Z","shell.execute_reply":"2024-07-03T03:13:09.099448Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport random\n\ndef stratified_split(data_indices, dataset_targets, labels, num_splits = 5, overlap_percent = 0.1):\n\n    stratified_splits = []\n    remaining_splits = []\n    unique_classes = np.unique(labels)\n\n    # Prepare lists to hold split indices and remaining indices\n    split_indices = [[] for _ in range(num_splits)]\n    remaining_indices_per_split = [[] for _ in range(num_splits)]\n    overlap_count = 0  # as per your original settings where overlap_count calculation was commented out\n\n    # Split indices by class\n    for cls in unique_classes:\n        cls_indices = [i for i in data_indices if dataset_targets[i] == int(cls)]\n        random.shuffle(cls_indices)\n        split_cls_indices = np.array_split(cls_indices, num_splits)\n        for i in range(num_splits):\n            split_indices[i].extend(split_cls_indices[i])\n            # Handle overlap if required\n            if overlap_count > 0:\n                remaining_indices = list(set(cls_indices) - set(split_cls_indices[i]))\n                overlap_indices = random.sample(remaining_indices, min(overlap_count, len(remaining_indices)))\n                split_indices[i].extend(overlap_indices)\n\n    # Calculate indices not included in each split\n    for i in range(num_splits):\n        all_shadow_indices = set(split_indices[i])\n        remaining_indices_per_split[i] = list(set(data_indices) - all_shadow_indices)\n\n    return split_indices\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:13:11.715399Z","iopub.execute_input":"2024-07-03T03:13:11.715787Z","iopub.status.idle":"2024-07-03T03:13:11.725318Z","shell.execute_reply.started":"2024-07-03T03:13:11.715756Z","shell.execute_reply":"2024-07-03T03:13:11.724329Z"},"trusted":true},"execution_count":155,"outputs":[]},{"cell_type":"markdown","source":"This function underpins sophisticated attack strategies by enabling precise and stratified training of shadow models, which are instrumental in both model shadowing and membership inference attacks. This underscores the need for robust security measures and ethical considerations in machine learning applications.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Subset, DataLoader\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import Adam\n\ndef train_shadow_models(train_dataset, device, split_indices, num_models, epochs):\n\n    shadow_models = []\n    shadow_model_indices = []\n    \n    for model_index in range(num_models):\n        indices_for_model = split_indices[model_index]\n        shadow_model_indices.append(indices_for_model)\n        model_specific_dataset = Subset(train_dataset, indices_for_model)\n        \n        model_loader = DataLoader(model_specific_dataset, batch_size=64, shuffle=True)\n\n        # Initialize a new model for each set of indices\n        model = CIFAR10Classifier().to(device)\n        optimizer = Adam(model.parameters(), lr=0.001)\n        criterion = CrossEntropyLoss()\n\n        # Train the model\n        for epoch in range(epochs):\n            model.train()\n            total_loss = 0\n            for inputs, labels in model_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n\n            print(f\"Shadow model {model_index}, Epoch {epoch}, Loss: {total_loss/len(model_loader)}\")\n\n        shadow_models.append(model)\n\n    return shadow_models, shadow_model_indices","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:13:14.886149Z","iopub.execute_input":"2024-07-03T03:13:14.886750Z","iopub.status.idle":"2024-07-03T03:13:14.897031Z","shell.execute_reply.started":"2024-07-03T03:13:14.886716Z","shell.execute_reply":"2024-07-03T03:13:14.896009Z"},"trusted":true},"execution_count":156,"outputs":[]},{"cell_type":"markdown","source":"Also known as model stealing, here the attacker aims to create a new model that replicates the functionality of the target model as closely as possible, without having access to the target model's actual data or architecture.","metadata":{}},{"cell_type":"code","source":"def update_shadow_model_list(shadow_model_list, model_index, new_model):\n\n    if 0 <= model_index < len(shadow_model_list):\n        shadow_model_list[model_index] = new_model\n    else:\n        raise IndexError(\"The specified index is out of the range of the shadow model list.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:13:18.383358Z","iopub.execute_input":"2024-07-03T03:13:18.383773Z","iopub.status.idle":"2024-07-03T03:13:18.388480Z","shell.execute_reply.started":"2024-07-03T03:13:18.383744Z","shell.execute_reply":"2024-07-03T03:13:18.387609Z"},"trusted":true},"execution_count":157,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset\nimport random\n\ndef prepare_membership_attack_data(shadow_models, shadow_indices_list, train_dataset, test_dataset, device, num_classes = 10):\n\n    attack_outputs = [[] for _ in range(num_classes)]\n    attack_labels = [[] for _ in range(num_classes)]\n\n    for cls in range(num_classes):\n        print(\"Class =\", cls)\n        for shadow_idx, shadow_model in enumerate(shadow_models):\n            print(\"Shadow model =\", shadow_idx + 1)\n            shadow_model.eval()\n\n            # Indices for shadow training data of the current class\n            shadow_train_indices = [idx for idx in shadow_indices_list[shadow_idx] if train_dataset.targets[idx] == cls]\n            shadow_train_loader = DataLoader(Subset(train_dataset, shadow_train_indices), batch_size=64, shuffle=False)\n\n            # Indices for test data of the same class\n            test_indices_cls = [idx for idx, label in enumerate(test_dataset.targets) if label == cls]\n            random.shuffle(test_indices_cls)\n            test_indices_cls = test_indices_cls[:len(shadow_train_indices)]\n            test_data_loader = DataLoader(Subset(test_dataset, test_indices_cls), batch_size=64, shuffle=False)\n\n            # Collect data from shadow model for training indices\n            for inputs, _ in shadow_train_loader:\n                inputs = inputs.to(device)\n                with torch.no_grad():\n                    outputs = shadow_model(inputs)\n                    probabilities = F.softmax(outputs, dim=1)\n                    for prob in probabilities:\n                        attack_outputs[cls].append(prob.cpu().numpy())\n                        attack_labels[cls].append(1)  # Label as in the training set\n\n            # Collect data from shadow model for test indices\n            for inputs, _ in test_data_loader:\n                inputs = inputs.to(device)\n                with torch.no_grad():\n                    outputs = shadow_model(inputs)\n                    probabilities = F.softmax(outputs, dim=1)\n                    for prob in probabilities:\n                        attack_outputs[cls].append(prob.cpu().numpy())\n                        attack_labels[cls].append(0)  # Label as not in the training set\n\n    return attack_outputs, attack_labels","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:13:20.702184Z","iopub.execute_input":"2024-07-03T03:13:20.702886Z","iopub.status.idle":"2024-07-03T03:13:20.715121Z","shell.execute_reply.started":"2024-07-03T03:13:20.702854Z","shell.execute_reply":"2024-07-03T03:13:20.714127Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"markdown","source":"This setup prepares data that can be used to analyze how well shadow models can differentiate between their training data and unseen test data, which is a crucial part of conducting effective membership inference attack","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef train_membership_attack_models(attack_data_by_class, attack_labels_by_class, num_classes=10, num_epochs=10):\n\n    trained_attack_models = []\n    for cls in range(num_classes):\n        # Convert data for this class to numpy arrays for training\n        class_attack_data = np.array(attack_data_by_class[cls])\n        class_attack_labels = np.array(attack_labels_by_class[cls])\n\n        # Initialize and train a random forest classifier as the attack model\n        attack_model = RandomForestClassifier(n_estimators=100)\n        attack_model.fit(class_attack_data.reshape(len(class_attack_data), -1), class_attack_labels)\n\n        # Store the trained model\n        trained_attack_models.append(attack_model)\n\n    return trained_attack_models\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:13:23.955115Z","iopub.execute_input":"2024-07-03T03:13:23.955948Z","iopub.status.idle":"2024-07-03T03:13:23.962475Z","shell.execute_reply.started":"2024-07-03T03:13:23.955915Z","shell.execute_reply":"2024-07-03T03:13:23.961454Z"},"trusted":true},"execution_count":159,"outputs":[]},{"cell_type":"markdown","source":"Calling the function with prepared attack data and labels, which might come from a membership inference preparation process or other data gathering method designed to analyze model vulnerability.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef predict_membership_from_models(target_model, test_loader, attack_models, device):\n\n    predictions = []\n    target_model.eval()  # Move the model to the correct device and set to evaluation mode\n    softmax_outputs = []\n\n    # First, collect softmax outputs from the target model\n    for inputs, _ in test_loader:\n        inputs = inputs.to(device)\n        with torch.no_grad():\n            outputs = target_model(inputs)\n            probabilities = F.softmax(outputs, dim=1)\n            softmax_outputs.append(probabilities.cpu().numpy())\n\n    # Second, predict membership using the softmax outputs\n    for i, (_, labels) in enumerate(test_loader.dataset):\n        cls = labels\n        \n        attack_model = attack_models[cls]\n        input_data = softmax_outputs[i].reshape(1, -1)\n        pred = attack_model.predict(input_data)\n        predictions.append(pred[0])\n\n    return predictions\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:13:25.885190Z","iopub.execute_input":"2024-07-03T03:13:25.885667Z","iopub.status.idle":"2024-07-03T03:13:25.893083Z","shell.execute_reply.started":"2024-07-03T03:13:25.885634Z","shell.execute_reply":"2024-07-03T03:13:25.892179Z"},"trusted":true},"execution_count":160,"outputs":[]},{"cell_type":"markdown","source":"This function is now independent and can be easily integrated into various machine learning pipelines or used for research purposes, especially in studies related to model privacy and security.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n\ndef evaluate_membership_attack_models(target_model, train_dataset, attack_models, train_indices, device):\n\n    # Load the subset of training data for which to test membership inference\n#     subset_loader = DataLoader(Subset(train_dataset, train_indices), batch_size=64, shuffle=False)\n#     DataLoader(Subset(train_dataset, train_indices))\n    \n    # Predict membership using the provided attack models\n    attack_predictions = predict_membership_from_models(target_model, DataLoader(Subset(train_dataset, train_indices)), attack_models, device)\n    \n    # All these data points are members of the training set\n    true_labels = [1] * len(train_indices)\n    \n    # Calculate various metrics\n    accuracy = accuracy_score(true_labels, attack_predictions)\n    cm = confusion_matrix(true_labels, attack_predictions)\n    precision = precision_score(true_labels, attack_predictions)\n    recall = recall_score(true_labels, attack_predictions)\n    f1 = f1_score(true_labels, attack_predictions)\n\n    # Print the metrics\n    print(f'Confusion Matrix:\\n{cm}')\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n\n    # Optionally, return the accuracy or all metrics\n    return accuracy  # You can also return a dictionary of all metrics if needed\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:20:10.926340Z","iopub.execute_input":"2024-07-03T03:20:10.926740Z","iopub.status.idle":"2024-07-03T03:20:10.935312Z","shell.execute_reply.started":"2024-07-03T03:20:10.926710Z","shell.execute_reply":"2024-07-03T03:20:10.934311Z"},"trusted":true},"execution_count":170,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Subset, TensorDataset\n\nassured_indices = filter_assured_indices(train_loader, model, device, batch_size = 64, threshold=0.9)\nlabels = get_labels_from_indices(trainset, assured_indices)\n\nsplit_indices = stratified_split(assured_indices, trainset.targets, labels, num_splits = 5, overlap_percent = 0.1)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:13:32.673455Z","iopub.execute_input":"2024-07-03T03:13:32.674136Z","iopub.status.idle":"2024-07-03T03:13:35.835773Z","shell.execute_reply.started":"2024-07-03T03:13:32.674104Z","shell.execute_reply":"2024-07-03T03:13:35.834727Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"code","source":"shadow_models, shadow_model_indices = train_shadow_models(trainset, device, split_indices, num_models = 5, epochs = 10)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:13:38.531421Z","iopub.execute_input":"2024-07-03T03:13:38.532433Z","iopub.status.idle":"2024-07-03T03:13:42.919926Z","shell.execute_reply.started":"2024-07-03T03:13:38.532399Z","shell.execute_reply":"2024-07-03T03:13:42.918924Z"},"trusted":true},"execution_count":163,"outputs":[{"name":"stdout","text":"Shadow model 0, Epoch 0, Loss: 2.283633550008138\nShadow model 0, Epoch 1, Loss: 2.2224972248077393\nShadow model 0, Epoch 2, Loss: 2.126007596651713\nShadow model 0, Epoch 3, Loss: 2.017794370651245\nShadow model 0, Epoch 4, Loss: 1.997313400109609\nShadow model 0, Epoch 5, Loss: 1.7687914570172627\nShadow model 0, Epoch 6, Loss: 1.763733685016632\nShadow model 0, Epoch 7, Loss: 1.7548107107480366\nShadow model 0, Epoch 8, Loss: 1.626665512720744\nShadow model 0, Epoch 9, Loss: 1.7539088726043701\nShadow model 1, Epoch 0, Loss: 2.332929086685181\nShadow model 1, Epoch 1, Loss: 2.196032238006592\nShadow model 1, Epoch 2, Loss: 2.0960459232330324\nShadow model 1, Epoch 3, Loss: 1.9966496467590331\nShadow model 1, Epoch 4, Loss: 1.8836265802383423\nShadow model 1, Epoch 5, Loss: 1.8177685737609863\nShadow model 1, Epoch 6, Loss: 1.6711799144744872\nShadow model 1, Epoch 7, Loss: 1.60203857421875\nShadow model 1, Epoch 8, Loss: 1.4851868152618408\nShadow model 1, Epoch 9, Loss: 1.3801929235458374\nShadow model 2, Epoch 0, Loss: 2.352371263504028\nShadow model 2, Epoch 1, Loss: 2.2256827354431152\nShadow model 2, Epoch 2, Loss: 2.156287336349487\nShadow model 2, Epoch 3, Loss: 2.0781723976135256\nShadow model 2, Epoch 4, Loss: 2.0150201082229615\nShadow model 2, Epoch 5, Loss: 1.90487699508667\nShadow model 2, Epoch 6, Loss: 1.8034350395202636\nShadow model 2, Epoch 7, Loss: 1.72066068649292\nShadow model 2, Epoch 8, Loss: 1.6505938291549682\nShadow model 2, Epoch 9, Loss: 1.5836469650268554\nShadow model 3, Epoch 0, Loss: 2.3413464546203615\nShadow model 3, Epoch 1, Loss: 2.2012030601501467\nShadow model 3, Epoch 2, Loss: 2.06702241897583\nShadow model 3, Epoch 3, Loss: 1.9826533555984498\nShadow model 3, Epoch 4, Loss: 1.955710196495056\nShadow model 3, Epoch 5, Loss: 1.8872612237930297\nShadow model 3, Epoch 6, Loss: 1.7902447700500488\nShadow model 3, Epoch 7, Loss: 1.6713767528533936\nShadow model 3, Epoch 8, Loss: 1.5867154359817506\nShadow model 3, Epoch 9, Loss: 1.4924161672592162\nShadow model 4, Epoch 0, Loss: 2.313416051864624\nShadow model 4, Epoch 1, Loss: 2.208854627609253\nShadow model 4, Epoch 2, Loss: 2.147066259384155\nShadow model 4, Epoch 3, Loss: 2.047742486000061\nShadow model 4, Epoch 4, Loss: 1.9547101736068726\nShadow model 4, Epoch 5, Loss: 1.8819597244262696\nShadow model 4, Epoch 6, Loss: 1.7971590280532836\nShadow model 4, Epoch 7, Loss: 1.7232504844665528\nShadow model 4, Epoch 8, Loss: 1.638524055480957\nShadow model 4, Epoch 9, Loss: 1.565687370300293\n","output_type":"stream"}]},{"cell_type":"code","source":"attack_outputs, attack_labels = prepare_membership_attack_data(shadow_models, shadow_model_indices, trainset, testset, device, num_classes = 10)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:13:46.034420Z","iopub.execute_input":"2024-07-03T03:13:46.035006Z","iopub.status.idle":"2024-07-03T03:13:47.092645Z","shell.execute_reply.started":"2024-07-03T03:13:46.034977Z","shell.execute_reply":"2024-07-03T03:13:47.091640Z"},"trusted":true},"execution_count":164,"outputs":[{"name":"stdout","text":"Class = 0\nShadow model = 1\nShadow model = 2\nShadow model = 3\nShadow model = 4\nShadow model = 5\nClass = 1\nShadow model = 1\nShadow model = 2\nShadow model = 3\nShadow model = 4\nShadow model = 5\nClass = 2\nShadow model = 1\nShadow model = 2\nShadow model = 3\nShadow model = 4\nShadow model = 5\nClass = 3\nShadow model = 1\nShadow model = 2\nShadow model = 3\nShadow model = 4\nShadow model = 5\nClass = 4\nShadow model = 1\nShadow model = 2\nShadow model = 3\nShadow model = 4\nShadow model = 5\nClass = 5\nShadow model = 1\nShadow model = 2\nShadow model = 3\nShadow model = 4\nShadow model = 5\nClass = 6\nShadow model = 1\nShadow model = 2\nShadow model = 3\nShadow model = 4\nShadow model = 5\nClass = 7\nShadow model = 1\nShadow model = 2\nShadow model = 3\nShadow model = 4\nShadow model = 5\nClass = 8\nShadow model = 1\nShadow model = 2\nShadow model = 3\nShadow model = 4\nShadow model = 5\nClass = 9\nShadow model = 1\nShadow model = 2\nShadow model = 3\nShadow model = 4\nShadow model = 5\n","output_type":"stream"}]},{"cell_type":"code","source":"trained_attack_models = train_membership_attack_models(attack_outputs, attack_labels, num_classes=10, num_epochs=10)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:13:49.764762Z","iopub.execute_input":"2024-07-03T03:13:49.765110Z","iopub.status.idle":"2024-07-03T03:13:52.071913Z","shell.execute_reply.started":"2024-07-03T03:13:49.765081Z","shell.execute_reply":"2024-07-03T03:13:52.071163Z"},"trusted":true},"execution_count":165,"outputs":[]},{"cell_type":"code","source":"from joblib import dump\n\nfor idx, model in enumerate(trained_attack_models):\n    dump(model, f'/kaggle/working/trained_attack_model_{idx}.joblib')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T02:52:01.725247Z","iopub.execute_input":"2024-07-03T02:52:01.725610Z","iopub.status.idle":"2024-07-03T02:52:02.352567Z","shell.execute_reply.started":"2024-07-03T02:52:01.725581Z","shell.execute_reply":"2024-07-03T02:52:02.351773Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"from joblib import load\n\n# If you saved each model separately:\ntrained_attack_models = []\nfor idx in range(number_of_models):  # Replace `number_of_models` with the actual number\n    model = load(f'trained_attack_model_{idx}.joblib')\n    trained_attack_models.append(model)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_model = CIFAR10Classifier()\nstate_dict = torch.load(\"/kaggle/input/phase1/model_state_dict.pth\", map_location=device)\nnew_state_dict = {key.replace('_module.', ''): value for key, value in state_dict.items()}\ntarget_model.load_state_dict(new_state_dict)\ntarget_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:17:03.234257Z","iopub.execute_input":"2024-07-03T03:17:03.235002Z","iopub.status.idle":"2024-07-03T03:17:03.261191Z","shell.execute_reply.started":"2024-07-03T03:17:03.234967Z","shell.execute_reply":"2024-07-03T03:17:03.260317Z"},"trusted":true},"execution_count":167,"outputs":[{"execution_count":167,"output_type":"execute_result","data":{"text/plain":"CIFAR10Classifier(\n  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n  (dropout1): Dropout2d(p=0.25, inplace=False)\n  (dropout2): Dropout2d(p=0.5, inplace=False)\n  (fc1): Linear(in_features=6272, out_features=64, bias=True)\n  (fc2): Linear(in_features=64, out_features=10, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"train_indices = torch.randperm(len(trainset))[:10000]\n\n# Evaluate attack models\nattack_accuracy = evaluate_membership_attack_models(target_model, trainset, trained_attack_models, train_indices, device)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:20:15.131274Z","iopub.execute_input":"2024-07-03T03:20:15.131735Z","iopub.status.idle":"2024-07-03T03:21:21.208928Z","shell.execute_reply.started":"2024-07-03T03:20:15.131697Z","shell.execute_reply":"2024-07-03T03:21:21.208008Z"},"trusted":true},"execution_count":171,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n  warnings.warn(warn_msg)\n","output_type":"stream"},{"name":"stdout","text":"Confusion Matrix:\n[[   0    0]\n [3498 6502]]\nPrecision: 1.0000\nRecall: 0.6502\nF1 Score: 0.7880\nAccuracy: 0.6502\n","output_type":"stream"}]}]}